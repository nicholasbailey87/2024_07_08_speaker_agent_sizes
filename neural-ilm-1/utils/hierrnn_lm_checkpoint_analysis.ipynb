{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from mll import hierrnn_lm, nospaces_dataset, run_mem_recv, run_mem_send\n",
    "\n",
    "# model_file = '../models/hierrnn_lm_mlb9_hughtok1_20190120_173425.dat.22k'\n",
    "model_file = '../tmp/hierrnn_lm_mlb10_hughtok1_20190120_202735.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... we need to:\n",
    "- load the model file\n",
    "- reconstruct the model\n",
    "- sample some words from the dataset\n",
    "- pass through the model\n",
    "- look at the resulting stopness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open(model_file, 'rb') as f:\n",
    "    state_dict = torch.load(f)\n",
    "print('state_dict.keys()', state_dict.keys())\n",
    "print('episode', state_dict['episode'])\n",
    "\n",
    "p = params = state_dict['params']\n",
    "print('params', params)\n",
    "\n",
    "dataset = nospaces_dataset.Dataset(\n",
    "    in_textfile=p.in_textfile\n",
    ")\n",
    "\n",
    "hier_enc_dec = hierrnn_lm.HierarchicalEncoderDecoder(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embedding_size=p.embedding_size,\n",
    "    rnn_type=p.rnn_type,\n",
    "    dropout=0\n",
    ")\n",
    "hier_enc_dec.load_state_dict(state_dict['hier_enc_dec_state'])\n",
    "print('loaded model, and dataset')\n",
    "\n",
    "encode_len = 50\n",
    "decode_len = 15\n",
    "\n",
    "num_its = 16\n",
    "\n",
    "encoder_sum_by_word_length = {}\n",
    "encoder_count_by_word_length = defaultdict(int)\n",
    "\n",
    "decoder_sum_by_word_length = {}\n",
    "decoder_count_by_word_length = defaultdict(int)\n",
    "\n",
    "for it in range(num_its):\n",
    "    print('.', end='', flush=True)\n",
    "    sample = dataset.sample(batch_size=p.batch_size, encode_len=encode_len, decode_len=decode_len)\n",
    "    encode_chars_t, decode_chars_t, encode_chars_l, decode_chars_l = map(sample.__getitem__, [\n",
    "        'encode_chars_t', 'decode_chars_t', 'encode_chars_l', 'decode_chars_l',\n",
    "    ])\n",
    "    encode_words_l, decode_words_l, encode_lens_t, decode_lens_t = map(sample.__getitem__, [\n",
    "        'encode_words_l', 'decode_words_l', 'encode_lens_t', 'decode_lens_t'\n",
    "    ])\n",
    "    print(encode_chars_l[0])\n",
    "\n",
    "    utts_out_logits, enc_stopness, dec_stopness = hier_enc_dec(sample['encode_chars_t'], decoder_utt_len=15)\n",
    "\n",
    "    def inc_sums(sum_by_word_length, count_by_word_length, words_l, chars_l, lens_t, stopness, justify):\n",
    "        batch_size = len(words_l)\n",
    "        max_len = stopness.size(0)\n",
    "        for n in range(batch_size):\n",
    "            if justify == 'right':\n",
    "                padding = max_len - lens_t[n].item()\n",
    "                pos = padding\n",
    "            elif justify == 'left':\n",
    "                pos = 0\n",
    "            else:\n",
    "                raise Exception('justify ' + justify + ' not recognized')\n",
    "            for i, word in enumerate(words_l[n]):\n",
    "                length = len(word)\n",
    "#                 assert word == chars_l[n][pos:pos + length]\n",
    "                stopness_t = stopness[pos:pos + length, n]\n",
    "                count_by_word_length[length] += 1\n",
    "                if length in sum_by_word_length:\n",
    "                    sum_by_word_length[length] += stopness_t\n",
    "                else:\n",
    "                    sum_by_word_length[length] = stopness_t\n",
    "                pos += length\n",
    "\n",
    "    inc_sums(\n",
    "        sum_by_word_length=encoder_sum_by_word_length,\n",
    "        count_by_word_length=encoder_count_by_word_length,\n",
    "        words_l=encode_words_l,\n",
    "        chars_l=encode_chars_l,\n",
    "        lens_t=encode_lens_t,\n",
    "        stopness=enc_stopness,\n",
    "        justify='right'\n",
    "    )\n",
    "    inc_sums(\n",
    "        sum_by_word_length=decoder_sum_by_word_length,\n",
    "        count_by_word_length=decoder_count_by_word_length,\n",
    "        words_l=decode_words_l,\n",
    "        chars_l=decode_chars_l,\n",
    "        lens_t=decode_lens_t,\n",
    "        stopness=dec_stopness,\n",
    "        justify='left'\n",
    "    )\n",
    "\n",
    "# print(encoder_count_by_word_length)\n",
    "\n",
    "print('')\n",
    "print('encoder')\n",
    "for length in sorted(encoder_count_by_word_length.keys()):\n",
    "    this_stopness = encoder_sum_by_word_length[length] / encoder_count_by_word_length[length]\n",
    "    this_stopness = (this_stopness * 100).int()\n",
    "    print('%i samples' % encoder_count_by_word_length[length], this_stopness.tolist())\n",
    "\n",
    "print('')\n",
    "print('decoder')\n",
    "for length in sorted(decoder_count_by_word_length.keys()):\n",
    "    this_stopness = decoder_sum_by_word_length[length] / decoder_count_by_word_length[length]\n",
    "    this_stopness = (this_stopness * 100).int()\n",
    "    print('%i samples' % decoder_count_by_word_length[length], this_stopness.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
