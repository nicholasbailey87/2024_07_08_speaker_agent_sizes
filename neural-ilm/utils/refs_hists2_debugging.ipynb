{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics.cluster\n",
    "\n",
    "import torch\n",
    "\n",
    "def create_ami_matrix(gnd, pred):\n",
    "    \"\"\"\n",
    "    assumptions:\n",
    "    - gnd and pred are 2d matrices\n",
    "    - gnd is [U_gnd][N]\n",
    "    - pred is [U_pred][N]\n",
    "    \"\"\"\n",
    "    U_gnd = gnd.size(0)\n",
    "    U_pred = pred.size(0)\n",
    "\n",
    "    ami_matrix = torch.zeros(U_pred, U_gnd)\n",
    "    for i in range(U_pred):\n",
    "        for j in range(U_gnd):\n",
    "            AMI = sklearn.metrics.cluster.adjusted_mutual_info_score(\n",
    "                labels_true=gnd[j].numpy(),\n",
    "                labels_pred=pred[i].numpy()\n",
    "            )\n",
    "            ami_matrix[i, j] = AMI\n",
    "    return ami_matrix\n",
    "\n",
    "def run(filename):\n",
    "    shutil.copyfile(f'../{filename}', '/tmp/foo.h5')\n",
    "    f = h5py.File('/tmp/foo.h5', 'r')\n",
    "\n",
    "    hypotheses_train = torch.from_numpy(f['hypotheses_train'][:])\n",
    "    hypotheses_gnd_train = torch.from_numpy(f['gnd_hypotheses_train'][:].astype(np.uint8)).long()\n",
    "    dsrefs_train = torch.from_numpy(f['dsrefs_train'][:].astype(np.uint8)).long()\n",
    "    resdicts = f['resdicts']\n",
    "\n",
    "    meta = json.loads(f['meta'][0])\n",
    "    ref = meta['ref']\n",
    "    print(ref)\n",
    "#     print('meta', json.dumps(meta, indent=2))\n",
    "    dsrefs = meta['ds_refs']\n",
    "    \n",
    "    U_gnd = hypotheses_gnd_train.size(0)\n",
    "    U_pred = hypotheses_train.size(0)\n",
    "\n",
    "    N = dsrefs_train.size(0)\n",
    "\n",
    "    render_start_id = len(resdicts) - 1\n",
    "    render_end_id_excl = len(resdicts)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    num_dsrefs = dsrefs_train.max().item() + 1\n",
    "    for dsref in range(num_dsrefs):\n",
    "        num_renders = render_end_id_excl - render_start_id\n",
    "        amis = torch.zeros((num_renders, U_pred, U_gnd), dtype=torch.float32)\n",
    "        for render_id in range(render_start_id, render_end_id_excl):\n",
    "            b_start = render_id * batch_size\n",
    "            b_end = b_start + batch_size\n",
    "            _hypotheses_train = hypotheses_train[:, b_start:b_end]\n",
    "            _hypotheses_gnd_train = hypotheses_gnd_train[:, b_start:b_end]\n",
    "            _dsrefs_train = dsrefs_train[b_start:b_end]\n",
    "\n",
    "            dsref_idxes = (_dsrefs_train == dsref).view(-1).nonzero().view(-1).long()\n",
    "            _hypotheses_train = _hypotheses_train[:, dsref_idxes]\n",
    "#             _hypotheses_train.fill_(0)\n",
    "#             _hypotheses_train[:, :, 0] = 1\n",
    "#             _hypotheses_train.uniform_(0, 1)\n",
    "            _hypotheses_gnd_train = _hypotheses_gnd_train[:, dsref_idxes]\n",
    "            \n",
    "            _, _pred = _hypotheses_train.max(dim=-1)\n",
    "            print('_hypotheses_gnd_train', _hypotheses_gnd_train)\n",
    "            print('_pred', _pred)\n",
    "            ami_matrix = create_ami_matrix(\n",
    "                gnd=_hypotheses_gnd_train,\n",
    "                pred=_pred\n",
    "            )\n",
    "            print('ami_matrix', ami_matrix)\n",
    "            plt.imshow(ami_matrix, vmin=0, vmax=1)\n",
    "            plt.show()\n",
    "            amis[render_id - render_start_id] = ami_matrix\n",
    "\n",
    "        resdict_start = json.loads(resdicts[render_start_id])\n",
    "        episode_start = resdict_start['episode']\n",
    "        resdict_final = json.loads(resdicts[-1])\n",
    "        episode_final = resdict_final['episode']\n",
    "\n",
    "#         plt.figure(figsize=(10.0 * U_gnd, 0.15 * num_renders))\n",
    "        for u_gnd in range(U_gnd):\n",
    "            plt.figure(figsize=(30, 0.2))\n",
    "            plt.cla()\n",
    "#             plt.subplot(1, 5, u_gnd + 1)\n",
    "            ami = amis[:, :, u_gnd]\n",
    "#             print('u_gnd', u_gnd, 'ami.size()', ami.size())\n",
    "            print('min', ami.min().item(), 'max', ami.max().item())\n",
    "            plt.imshow(\n",
    "                ami.transpose(0, 1).numpy(),\n",
    "                extent=[episode_start, episode_final, 0, U_pred],\n",
    "                interpolation='none',\n",
    "#                 vmin=0,\n",
    "#                 vmax=1\n",
    "            )\n",
    "            plt.title(f'{ref} episodes={episode_final} dsref={dsrefs[dsref]} u_gnd={u_gnd}')\n",
    "            plt.show()\n",
    "    f.close()\n",
    "\n",
    "filenames = [\n",
    "#     'hists/hypprop_with_predictor_eeb99_hughtok1_20180921_191657.h5',\n",
    "#     'hists/hypprop_with_predictor_eeb100_hughtok2_20180921_193244.h5',\n",
    "#     'hists/hypprop_with_predictor_eeb101_hughtok3_20180921_193516.h5',\n",
    "#     'hists/hypprop_with_predictor_eeb102_hughtok4_20180921_193310.h5',\n",
    "\n",
    "#     'hists/hypprop_with_predictor_eeb103_hughtok1_20180921_212947.h5',\n",
    "\n",
    "#     'hists/hypprop_with_predictor_eeb105_hughtok5_20180921_230949.h5',\n",
    "#     'hists/hypprop_with_predictor_eeb106_hughtok6_20180921_231010.h5',\n",
    "\n",
    "#     'hists/hypprop_with_predictor_eeb107_hughtok2_20180922_013551.h5',\n",
    "#     'hists/hypprop_with_predictor_eeb108_hughtok1_20180922_170013.h5',\n",
    "#     'hists/hypprop_with_predictor_eeb109_hughtok2_20180922_184537.h5',\n",
    "    'hists/hypprop_with_predictor_eeb113_hughtok6_20180922_211153.h5',\n",
    "]\n",
    "\n",
    "# filenames = \"\"\"\n",
    "# ../hists/hypprop_with_predictor_eeb133_hughtok4_20180925_082438.h5\n",
    "# ../hists/hypprop_with_predictor_eeb134_hughtok5_20180925_082440.h5\n",
    "# ../hists/hypprop_with_predictor_eeb135_hughtok6_20180925_082442.h5\n",
    "# ../hists/hypprop_with_predictor_eeb130_hughtok1_20180925_014352.h5\n",
    "# ../hists/hypprop_with_predictor_eeb131_hughtok2_20180925_014520.h5\n",
    "# ../hists/hypprop_with_predictor_eeb132_hughtok3_20180925_014550.h5\n",
    "# \"\"\".split('\\n')\n",
    "# filenames = [f.replace('../', '') for f in filenames if f != '']\n",
    "\n",
    "filenames = [\n",
    "#     'hists/hypprop_with_predictor_eeb128_hughtok7_20180924_145219.h5',\n",
    "#     'hists/hypprop_with_predictor_eeb129_hughtok1_20180924_145244.h5'\n",
    "    'hists/hypprop_with_predictor_eeb133_hughtok4_20180925_082438.h5'\n",
    "]\n",
    "for filename in filenames:\n",
    "    run(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
