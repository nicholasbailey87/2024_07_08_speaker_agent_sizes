{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5867761e-ec65-4ead-852b-183831bc3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1941a38-9621-46d8-a591-085b3a632721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc02874-0fa9-4a37-ac53-a0a07bb528be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb856e76-1074-406f-99df-535a9bcea753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\git_repos\\\\2024_05_12_speaker_agent_sizes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "original_working_directory = os.getcwd()\n",
    "original_working_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3c55ce-6af9-4c9b-ac2d-304a92c2c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(module):\n",
    "    parameter_count = 0\n",
    "    for params in module.parameters():\n",
    "        parameter_count += params.numel()\n",
    "    return (parameter_count, len(str(parameter_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee12fde-5f0f-415b-bbd5-3f96dcd48b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_sizes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd363ac-0b21-40b3-a8f3-7cc68ce38321",
   "metadata": {},
   "source": [
    "# 1. Natural language does not emerge ’naturally’ in multi-agent dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0be0b82-7524-4b57-8e5b-352e92b8a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "\t  hiddenSize : 50\n",
      "\t   embedSize : 20\n",
      "\t imgFeatSize : 20\n",
      "\t   qOutVocab : 3\n",
      "\t   aOutVocab : 4\n",
      "\t     dataset : data/64_synthetic.json\n",
      "\t     rlScale : 100.0\n",
      "\t   numRounds : 2\n",
      "\t    remember : False\n",
      "\t negFraction : 0.8\n",
      "\t   batchSize : 1000\n",
      "\t   numEpochs : 1000000\n",
      "\tlearningRate : 0.001\n",
      "\t      useGPU : False\n",
      "Answerer(\n",
      "  (inNet): Embedding(7, 20)\n",
      "  (outNet): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (imgNet): Embedding(12, 20)\n",
      "  (rnn): LSTMCell(80, 50)\n",
      ")\n",
      "Questioner(\n",
      "  (inNet): Embedding(16, 20)\n",
      "  (outNet): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (rnn): LSTMCell(20, 50)\n",
      "  (predictRNN): LSTMCell(20, 50)\n",
      "  (predictNet): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29885, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('lang-emerge')\n",
    "\n",
    "# script to train interactive bots in toy world\n",
    "# author: satwik kottur\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools, pdb, random, os\n",
    "import numpy as np\n",
    "from chatbots import Team\n",
    "from dataloader import Dataloader\n",
    "import options\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# read the command line options\n",
    "options = options.read()\n",
    "#------------------------------------------------------------------------\n",
    "# setup experiment and dataset\n",
    "#------------------------------------------------------------------------\n",
    "data = Dataloader(options)\n",
    "numInst = data.getInstCount()\n",
    "\n",
    "params = data.params\n",
    "# append options from options to params\n",
    "for key, value in options.items():\n",
    "  params[key] = value\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# build agents, and setup optmizer\n",
    "#------------------------------------------------------------------------\n",
    "team = Team(params)\n",
    "\n",
    "speaker = team.qBot\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece762b-3d72-4b4b-8410-106a75303878",
   "metadata": {},
   "source": [
    "## 2. Emergence of Grounded Compositional Language in Multi-Agent Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e940afa-0728-4935-8fa9-fde5fec80bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1870105, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('emergent-language/modules')\n",
    "\n",
    "from agent import AgentModule\n",
    "from configs import default_agent_config\n",
    "\n",
    "speaker = AgentModule(default_agent_config)\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02125c-fac8-43a6-b4e9-3483aee316d8",
   "metadata": {},
   "source": [
    "## 3. Emergence of Communication in an Interactive World with Consistent Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfbe0096-2fdd-42b2-ba8d-eb142364a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gym) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75478047-c791-49e5-aac6-83588a461779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45253, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('emergence-communication-cco/agent')\n",
    "\n",
    "import agent_type\n",
    "from vec_agent import VecAgent\n",
    "\n",
    "speaker = VecAgent(agent_type.AgentType.speaker, acting=False)\n",
    "\n",
    "params, order = count_parameters(speaker.model)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergence of Communication in an Interactive World with Consistent Speakers',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64496e-878b-4b0e-bd02-a71e737bb73a",
   "metadata": {},
   "source": [
    "## 4. Compositional Obverter Communication Learning From Raw Visual Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c156593c-a89e-4c07-b055-2dc5cc7f3f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64676, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('obverter')\n",
    "\n",
    "from model import ConvModel\n",
    "\n",
    "speaker = ConvModel(5)\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositional Obverter Communication Learning From Raw Visual Input',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28743794-029a-4269-93aa-1f4a13641e87",
   "metadata": {},
   "source": [
    "## 5. Emergence of Compositional Language with Deep Generational Transmission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b18d0f-2ff1-43fe-9a86-8b02ece38d4f",
   "metadata": {},
   "source": [
    "Install an old version of attrs so that we can install and run parlai this one time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dda0921-c4c2-4b9c-b031-9ad19fc81ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting attrs==20.2.0\n",
      "  Using cached attrs-20.2.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Using cached attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
      "Installing collected packages: attrs\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.2.0\n",
      "    Uninstalling attrs-22.2.0:\n",
      "      Successfully uninstalled attrs-22.2.0\n",
      "Successfully installed attrs-20.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jsonschema 4.19.2 requires attrs>=22.2.0, but you have attrs 20.2.0 which is incompatible.\n",
      "referencing 0.30.2 requires attrs>=22.2.0, but you have attrs 20.2.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install attrs==20.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75ca3401-c409-4516-aeb3-c04bfd817e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parlai in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (15.0.1)\n",
      "Requirement already satisfied: datasets<2.2.2,>=1.4.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.2.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.14 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.15.2)\n",
      "Requirement already satisfied: emoji in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.12.1)\n",
      "Requirement already satisfied: fairscale~=0.4.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.4.13)\n",
      "Requirement already satisfied: docformatter~=1.4.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.4)\n",
      "Requirement already satisfied: flake8-bugbear in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (24.4.26)\n",
      "Requirement already satisfied: flake8 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (7.0.0)\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.18.0)\n",
      "Requirement already satisfied: google-cloud-storage in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.14.0)\n",
      "Requirement already satisfied: iopath~=0.1.8 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.1.10)\n",
      "Requirement already satisfied: gitdb2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (4.0.2)\n",
      "Requirement already satisfied: GitPython in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (3.1.43)\n",
      "Requirement already satisfied: hydra-core>=1.1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.3.2)\n",
      "Requirement already satisfied: ipython in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (8.12.2)\n",
      "Requirement already satisfied: torch in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.3.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.18.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.4.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (3.8.1)\n",
      "Requirement already satisfied: omegaconf>=2.1.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.0.3)\n",
      "Requirement already satisfied: pytest-regressions in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.5.0)\n",
      "Requirement already satisfied: pytest in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (8.2.2)\n",
      "Requirement already satisfied: pexpect in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (4.9.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (10.3.0)\n",
      "Requirement already satisfied: py-gfm in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.0.0)\n",
      "Requirement already satisfied: py-rouge in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (6.0.1)\n",
      "Requirement already satisfied: pyzmq in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (25.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2024.5.15)\n",
      "Requirement already satisfied: myst-parser<1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.19.2)\n",
      "Requirement already satisfied: attrs~=20.2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (20.2.0)\n",
      "Requirement already satisfied: requests-mock in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.12.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.32.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.10.1)\n",
      "Requirement already satisfied: sh in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.0.7)\n",
      "Requirement already satisfied: sphinx-rtd-theme in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.0.0)\n",
      "Requirement already satisfied: sphinx-autodoc-typehints~=1.10.3 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.10.3)\n",
      "Requirement already satisfied: Sphinx~=5.1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (5.1.1)\n",
      "Requirement already satisfied: subword-nmt in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.3.8)\n",
      "Requirement already satisfied: tensorboardX<=2.5.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.5)\n",
      "Requirement already satisfied: tokenizers>=0.8.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.19.1)\n",
      "Requirement already satisfied: tomli>=2.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.0.1)\n",
      "Requirement already satisfied: torchtext in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.18.0)\n",
      "Requirement already satisfied: tornado in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (6.3.3)\n",
      "Requirement already satisfied: tqdm~=4.62.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (4.11.0)\n",
      "Requirement already satisfied: Unidecode in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.3.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.26.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.26.18)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.8.0)\n",
      "Requirement already satisfied: jsonlines in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (4.0.0)\n",
      "Requirement already satisfied: numpy~=1.23.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.23.5)\n",
      "Requirement already satisfied: markdown<=3.3.2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (3.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (3.1.4)\n",
      "Requirement already satisfied: ninja~=1.10.2.3 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (1.10.2.4)\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.8.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (3.20.3)\n",
      "Requirement already satisfied: contractions~=0.1.72 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (0.1.73)\n",
      "Requirement already satisfied: fsspec~=2022.2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2022.2.0)\n",
      "Requirement already satisfied: google-api-core<=2.11.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from parlai) (2.11.0)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from contractions~=0.1.72->parlai) (0.0.24)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (16.1.0)\n",
      "Requirement already satisfied: dill in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from datasets<2.2.2,>=1.4.1->parlai) (0.18.0)\n",
      "Requirement already satisfied: untokenize in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from docformatter~=1.4.0->parlai) (0.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-api-core<=2.11.0->parlai) (1.63.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-api-core<=2.11.0->parlai) (2.30.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from hydra-core>=1.1.0->parlai) (4.9.3)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from hydra-core>=1.1.0->parlai) (6.1.1)\n",
      "Requirement already satisfied: portalocker in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from iopath~=0.1.8->parlai) (2.8.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from myst-parser<1->parlai) (2.2.0)\n",
      "Requirement already satisfied: mdit-py-plugins~=0.3.4 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from myst-parser<1->parlai) (0.3.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from requests<3,>=2.21.0->parlai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from requests<3,>=2.21.0->parlai) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from requests<3,>=2.21.0->parlai) (2024.6.2)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (1.0.4)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (2.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (1.1.5)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (1.0.3)\n",
      "Requirement already satisfied: Pygments>=2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (2.15.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (2.2.0)\n",
      "Requirement already satisfied: babel>=1.3 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (2.11.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (0.7.13)\n",
      "Requirement already satisfied: imagesize in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (7.0.1)\n",
      "Requirement already satisfied: colorama>=0.3.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from Sphinx~=5.1.0->parlai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from jinja2->parlai) (2.1.3)\n",
      "Requirement already satisfied: six in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from tensorboardX<=2.5.0->parlai) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch->parlai) (3.15.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch->parlai) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch->parlai) (3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch->parlai) (2021.4.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from coloredlogs->parlai) (10.0)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from flake8->parlai) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from flake8->parlai) (2.11.1)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from flake8->parlai) (3.2.0)\n",
      "Requirement already satisfied: gitdb>=4.0.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gitdb2->parlai) (4.0.11)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-cloud-storage->parlai) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-cloud-storage->parlai) (2.7.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-cloud-storage->parlai) (1.5.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (3.0.43)\n",
      "Requirement already satisfied: stack-data in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from ipython->parlai) (5.14.3)\n",
      "Requirement already satisfied: click in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from nltk->parlai) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pandas->parlai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pandas->parlai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pandas->parlai) (2024.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pexpect->parlai) (0.7.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->parlai) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->parlai) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->parlai) (1.2.0)\n",
      "Requirement already satisfied: pytest-datadir>=1.2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest-regressions->parlai) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from scikit-learn->parlai) (3.5.0)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from sphinx-rtd-theme->parlai) (4.1)\n",
      "Requirement already satisfied: mock in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from subword-nmt->parlai) (5.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gitdb>=4.0.1->gitdb2->parlai) (5.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<=2.11.0->parlai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<=2.11.0->parlai) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<=2.11.0->parlai) (4.9)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->parlai) (3.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from importlib-metadata>=4.4->Sphinx~=5.1.0->parlai) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from jedi>=0.16->ipython->parlai) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser<1->parlai) (0.1.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->parlai) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->parlai) (2021.12.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->parlai) (0.2.5)\n",
      "Requirement already satisfied: anyascii in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from textsearch>=0.0.21->contractions~=0.1.72->parlai) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from textsearch>=0.0.21->contractions~=0.1.72->parlai) (2.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from aiohttp->datasets<2.2.2,>=1.4.1->parlai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from aiohttp->datasets<2.2.2,>=1.4.1->parlai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from aiohttp->datasets<2.2.2,>=1.4.1->parlai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from aiohttp->datasets<2.2.2,>=1.4.1->parlai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from aiohttp->datasets<2.2.2,>=1.4.1->parlai) (4.0.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from portalocker->iopath~=0.1.8->parlai) (305.1)\n",
      "Requirement already satisfied: executing in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from stack-data->ipython->parlai) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from stack-data->ipython->parlai) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from stack-data->ipython->parlai) (0.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from sympy->torch->parlai) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core<=2.11.0->parlai) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01fbc68f-b6f5-493c-a0d7-14c4aa5c1588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99255, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('evolang')\n",
    "\n",
    "from bots import Questioner\n",
    "\n",
    "params, order = count_parameters(Questioner(\n",
    "    {\n",
    "        # defaults taken from options.py \n",
    "        'q_out_vocab': 3,\n",
    "        'a_out_vocab': 4,\n",
    "        'task_vocab': 0, # This is a dummy value as I don't know what it is, but it doesn't appear to affect the size of the qbot\n",
    "        'embed_size': 20,\n",
    "        'hidden_size': 100,\n",
    "        'props': {'shapes': [0, 1, 2, 3], 'styles': [0, 1, 2, 3], 'colors': [0, 1, 2, 3]} # From the json in the datasets folder\n",
    "    }\n",
    "))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergence of Compositional Language with Deep Generational Transmission',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8c75f-a3e7-4e40-9b95-56c9f3bcc0f9",
   "metadata": {},
   "source": [
    "Reinstall newer version of attrs so that everything else can run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8de8fa9-87a8-409e-b9ca-87a957aadf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting attrs==22.2.0\n",
      "  Using cached attrs-22.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Installing collected packages: attrs\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.2.0\n",
      "    Uninstalling attrs-20.2.0:\n",
      "      Successfully uninstalled attrs-20.2.0\n",
      "Successfully installed attrs-22.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parlai 1.7.2 requires attrs~=20.2.0, but you have attrs 22.2.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install attrs==22.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f44e2-698c-4188-b75a-49635800ea39",
   "metadata": {},
   "source": [
    "## 6. The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc5397d8-4dc0-4390-8fbe-2446518134cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6264858, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on https://github.com/lychengrex/LeNet-5-Implementation-Using-Pytorch/blob/master/LeNet-5%20Implementation%20Using%20Pytorch.ipynb\n",
    "\n",
    "# The paper says they use an LSTM speaker like Havrylov and Titov (2017) \"Emergence of language with multi-agent game...\"\n",
    "# Havrylov and Titov say they use an LSTM with hidden size 512 and vocabulary 10000\n",
    "# And also a LeNet for embedding images\n",
    "\n",
    "class HavrylovNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(HavrylovNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            10, # LSTM input size is LeNet output size, i.e. 10\n",
    "            512\n",
    "        )\n",
    "        self.projector = nn.Linear(512, 10000)\n",
    "\n",
    "    def forward(self, img, wrd):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        return F.softmax(self.projector(self.rnn(x)))\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        '''\n",
    "        Get the number of features in a batch of tensors `x`.\n",
    "        '''\n",
    "        size = x.size()[1:]\n",
    "        return np.prod(size)\n",
    "\n",
    "params, order = count_parameters(torch.nn.Sequential(LeNet(), HavrylovNet()))\n",
    "parameter_sizes.append({\n",
    "    'title': 'The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fa5a3-83c6-40e6-a179-9d7c10a075a9",
   "metadata": {},
   "source": [
    "## 7. Ease-of-Teaching and Language Structure from Emergent Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4396d032-3a2d-48e0-a0ba-2c5f841fb39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46108, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Ease-of-teaching-and-language-structure/code')\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parse():\n",
    "    parser = argparse.ArgumentParser(description='Referential game settings')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='which gpu if we use gpu')\n",
    "    parser.add_argument('--fname', type=str, default='test', help='folder name to save results')\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--jupyter', action='store_true') \n",
    "    parser.add_argument('--slambda', type=float, default=0.1, help='speaker regularization hyperparameter')\n",
    "    parser.add_argument('--rlambda', type=float, default=0.1, help='listener regularization hyperparameter')\n",
    "    parser.add_argument('--receiverNum', type=int, default=1, help='number of listeners in the population')\n",
    "    parser.add_argument('--topk', type=int, default=3, help='number of top messages when we probe language')\n",
    "    parser.add_argument('--evaluateSize', type=int, default=1000, help='the batch size of test objects when not enumeration')\n",
    "    args_dict = vars(parser.parse_args([])) # convert python object to dict\n",
    "    return args_dict\n",
    "\n",
    "args = parse()  # parsed argument from CLI\n",
    "args['device'] = torch.device(\"cuda:\" + str(args['gpu']) if torch.cuda.is_available() else \"cpu\")\n",
    "if not os.path.exists(args['fname']):\n",
    "    os.makedirs(args['fname'])\n",
    "\n",
    "# dataset hyperparameters\n",
    "args['numColors'] = 8 \n",
    "args['numShapes'] = 4 \n",
    "args['attrSize'] = args['numColors'] + args['numShapes']  # colors + shapes\n",
    "\n",
    "# game settings\n",
    "args['vocabSize'] = 8\n",
    "args['messageLen'] = 2\n",
    "args['distractNum'] = 5  # including targets\n",
    "\n",
    "# training hyperparameters\n",
    "args['batchSize'] = 100  # total train data = batchSize * numIters\n",
    "args['sLearnRate'] = 0.001  \n",
    "args['rLearnRate'] = 0.001  \n",
    "\n",
    "args['trainIters'] = 300000 # training\n",
    "args['resetNum'] = 50  \n",
    "args['resetIter'] = args['trainIters'] // args['resetNum']  # life of a receiver: 6K\n",
    "args['deterResetNums'] = 30\n",
    "args['deterResetIter'] = 1000\n",
    "\n",
    "# population of receivers training\n",
    "args['population'] = False\n",
    "\n",
    "# model hyperparameters\n",
    "args['hiddenSize'] = 100 \n",
    "\n",
    "from models import Sender\n",
    "\n",
    "params, order = count_parameters(Sender(args))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Ease-of-Teaching and Language Structure from Emergent Communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a2a65-41f6-40d1-b78e-0e15e45e1aed",
   "metadata": {},
   "source": [
    "## 8. Compositional Languages Emerge in a Neural Iterated Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6172b5d-e6bd-45db-83a5-9772fd44aae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73872, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Neural_Iterated_Learning/models')\n",
    "\n",
    "if \"model\" in sys.modules:\n",
    "    del sys.modules[\"model\"] # we used model before from a different repository!\n",
    "from model import SpeakingAgent\n",
    "\n",
    "os.chdir('../utils')\n",
    "import conf\n",
    "\n",
    "params, order = count_parameters(SpeakingAgent())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositional Languages Emerge in a Neural Iterated Learning Model',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a2854-4372-4311-a565-9f8973e34ee5",
   "metadata": {},
   "source": [
    "## 9. Compositionality and Generalization in Emergent Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05c8e3a5-748b-4f0c-905b-fb58f44ca8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153600, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The paper says\n",
    "\n",
    "  Each input i of the reconstruction game is comprised of iatt attributes,\n",
    "  each with ival possible values. We let i_{att} range from 2 to 4 and i_{val}\n",
    "  from 4 to 100. We represent each attribute as a i_{val} one-hot vector. An\n",
    "  input i is given by the concatenation of its attributes. \n",
    "\n",
    "  ...\n",
    "\n",
    "  Both agents are implemented as single-layer GRU cells (Cho et al., 2014)\n",
    "  with hidden states of size 500.\n",
    "  \n",
    "  Sender encodes i in a message m of fixed length c_{len} as follows. First,\n",
    "  a linear layer maps the input vector into the initial hidden state of\n",
    "  Sender. Next, the message is generated symbol-by-symbol by sampling from a\n",
    "  Categorical distribution over the vocabulary cvoc, parameterized by a linear\n",
    "  mapping from Sender’s hidden state.\n",
    "\n",
    "  In practice, we fix [vocabulary size to 100].\n",
    "\"\"\"\n",
    "\n",
    "class CompGenNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(CompGenNet, self).__init__()\n",
    "        self.fc1   = nn.Linear(400, 500)\n",
    "        self.gru   = nn.GRU(100, 500)\n",
    "        self.fc2   = nn.Linear(500, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        hidden = self.fc1(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        return self.fc2(out)\n",
    "\n",
    "params, order = count_parameters(CompGenNet())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositionality and Generalization in Emergent Languages',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e4912-9131-49ed-b894-ad815b86623a",
   "metadata": {},
   "source": [
    "## 10. Co-evolution of language and agents in referential games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4ab363a-ff87-46a8-8793-2cedbcbe6449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af5969c6-3c98-4d07-b799-33ebd8cda360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1696909, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('cultural-evolution-engine/model')\n",
    "\n",
    "from ShapesModels import ShapesSender\n",
    "from visual_module import CNN\n",
    "\n",
    "cnn = CNN(\n",
    "    # paper says \"The linear layer which followed the convolutional layers had output dimensions of 512\"\n",
    "    n_out_features=512\n",
    ")\n",
    "\n",
    "sender = ShapesSender(vocab_size=5, output_len=5, sos_id=0)\n",
    "params, order = count_parameters(torch.nn.Sequential(cnn, sender))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Co-evolution of language and agents in referential games',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c347d-0c99-416f-8d4e-35d54247c1a2",
   "metadata": {},
   "source": [
    "## 11. Inductive Bias and Language Expressivity in Emergent Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a356a2b5-4df0-40e6-873a-ef6f0e76d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/EGG.git\n",
      "  Cloning https://github.com/facebookresearch/EGG.git to c:\\users\\nicho\\appdata\\local\\temp\\pip-req-build-sf6dwi8i\n",
      "  Resolved https://github.com/facebookresearch/EGG.git to commit f36d123af22eb0d127d2089c993b4eff8314a43d\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from EGG==0.1.0) (2.3.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from EGG==0.1.0) (0.18.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from EGG==0.1.0) (1.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from EGG==0.1.0) (1.23.5)\n",
      "Requirement already satisfied: pytest in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from EGG==0.1.0) (8.2.2)\n",
      "Collecting editdistance (from EGG==0.1.0)\n",
      "  Downloading editdistance-0.8.1-cp38-cp38-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting dataclasses (from EGG==0.1.0)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich (from EGG==0.1.0)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from EGG==0.1.0) (1.3.2)\n",
      "Collecting wandb (from EGG==0.1.0)\n",
      "  Downloading wandb-0.17.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting submitit (from EGG==0.1.0)\n",
      "  Downloading submitit-1.5.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch>=1.1.0->EGG==0.1.0) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch>=1.1.0->EGG==0.1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch>=1.1.0->EGG==0.1.0) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch>=1.1.0->EGG==0.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch>=1.1.0->EGG==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch>=1.1.0->EGG==0.1.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torch>=1.1.0->EGG==0.1.0) (2021.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->EGG==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->EGG==0.1.0) (23.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->EGG==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->EGG==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->EGG==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from pytest->EGG==0.1.0) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from rich->EGG==0.1.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from rich->EGG==0.1.0) (2.15.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from scikit-learn->EGG==0.1.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from scikit-learn->EGG==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from submitit->EGG==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from torchvision->EGG==0.1.0) (10.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->EGG==0.1.0)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (2.32.2)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->EGG==0.1.0)\n",
      "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb->EGG==0.1.0)\n",
      "  Downloading setproctitle-1.3.3-cp38-cp38-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from wandb->EGG==0.1.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb->EGG==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->EGG==0.1.0) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->EGG==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.1.0->EGG==0.1.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.1.0->EGG==0.1.0) (2021.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from requests<3,>=2.0.0->wandb->EGG==0.1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from requests<3,>=2.0.0->wandb->EGG==0.1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from requests<3,>=2.0.0->wandb->EGG==0.1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from requests<3,>=2.0.0->wandb->EGG==0.1.0) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from jinja2->torch>=1.1.0->EGG==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from sympy->torch>=1.1.0->EGG==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->EGG==0.1.0) (5.0.1)\n",
      "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Downloading editdistance-0.8.1-cp38-cp38-win_amd64.whl (79 kB)\n",
      "   ---------------------------------------- 0.0/79.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 79.6/79.6 kB ? eta 0:00:00\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Downloading submitit-1.5.1-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 74.7/74.7 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading wandb-0.17.1-py3-none-win_amd64.whl (6.7 MB)\n",
      "   ---------------------------------------- 0.0/6.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.4/6.7 MB 7.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.6/6.7 MB 6.8 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.9/6.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.2/6.7 MB 6.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.5/6.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.1/6.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.4/6.7 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.6/6.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.9/6.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.2/6.7 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.5/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.8/6.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.0/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.5/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.0/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.3/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.7/6.7 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
      "   ---------------------------------------- 0.0/289.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 289.6/289.6 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading setproctitle-1.3.3-cp38-cp38-win_amd64.whl (11 kB)\n",
      "Building wheels for collected packages: EGG\n",
      "  Building wheel for EGG (setup.py): started\n",
      "  Building wheel for EGG (setup.py): finished with status 'done'\n",
      "  Created wheel for EGG: filename=EGG-0.1.0-py3-none-any.whl size=171873 sha256=b0b0e59a1da006aaf5fd28c4720f702b36a59b8894a6a1d635dc263c1e91d3b8\n",
      "  Stored in directory: C:\\Users\\nicho\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-4_5yqlj2\\wheels\\1e\\5c\\fe\\9ef678a355cab3453ef54d7b0d89b754a74f7058e9f7bc801c\n",
      "Successfully built EGG\n",
      "Installing collected packages: dataclasses, submitit, setproctitle, sentry-sdk, editdistance, docker-pycreds, rich, wandb, EGG\n",
      "Successfully installed EGG-0.1.0 dataclasses-0.6 docker-pycreds-0.4.0 editdistance-0.8.1 rich-13.7.1 sentry-sdk-2.5.1 setproctitle-1.3.3 submitit-1.5.1 wandb-0.17.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/EGG.git 'C:\\Users\\nicho\\AppData\\Local\\Temp\\pip-req-build-sf6dwi8i'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/facebookresearch/EGG.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0482c667-2029-4171-a4fe-88a6f11ac3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443584, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('GameBias-EmeCom2020')\n",
    "\n",
    "from modules import DspritesSenderCNN\n",
    "\n",
    "params, order = count_parameters(DspritesSenderCNN())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Inductive Bias and Language Expressivity in Emergent Communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352296c-37cd-46dd-8ab8-1304a9ae36b0",
   "metadata": {},
   "source": [
    "## 12. Capacity, Bandwidth, and Compositionality in Emergent Language Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23f964bc-cfa4-4842-9331-f19d979430e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670982, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "os.chdir(original_working_directory)\n",
    "\n",
    "# Read in the file\n",
    "with open('cbc-emecom/main.py', 'r') as file:\n",
    "  filedata = file.read()\n",
    "\n",
    "# Replace the target string\n",
    "filedata = filedata.replace('parser.add_argument', '#')\n",
    "\n",
    "# Write the file out again\n",
    "with open('cbc-emecom/main.py', 'w') as file:\n",
    "  file.write(filedata)\n",
    "os.chdir(\"cbc-emecom\")\n",
    "\n",
    "# sys.argv = [\"main.py\", \"--num-binary-messages\", \"24\", \"--num-digits\", \"6\", \"--embedding-size-sender\", \"40\", \"--project-size-sender\", \"60\", \"--num-lstm-sender\", \"300\", \"--num-lstm-receiver\", \"325\", \"--embedding-size-receiver\", \"125\", \"--save-str\", \"<SAVE_STR>\"]\n",
    "\n",
    "from main import CompCap\n",
    "\n",
    "config = {\n",
    "    'device': device,\n",
    "    'num_binary_messages': 24,\n",
    "    'seed': 0,\n",
    "    \n",
    "    # problem size\n",
    "    'batch_size': 100,\n",
    "    'num_digits': 6,\n",
    "    'signature_size': 2,\n",
    "    \n",
    "    # network params\n",
    "    'embedding_size_sender': 40,\n",
    "    'project_size_sender': 60,\n",
    "    'num_lstm_sender': 300,\n",
    "    'num_lstm_receiver': 325,\n",
    "    'embedding_size_receiver': 125,\n",
    "    \n",
    "    # optimization params\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'output_loss_penalty': 1,\n",
    "    'weight_norm_penalty': 1e-4,\n",
    "    'temp': 1,\n",
    "    'max_iters': 200000,\n",
    "    'train_acc': 0.60,\n",
    "    'trainval_acc': 0.60,\n",
    "    # logging/printing\n",
    "    'trainval_interval': 50,\n",
    "    'model_dir': None,\n",
    "    'save_str': '',\n",
    "    'log_dir': \"./logs\",\n",
    "    'save_dir': \"./models\"\n",
    "}\n",
    "\n",
    "# parameter_count = 0\n",
    "\n",
    "capacity_bandwidth_compositionality = CompCap(config)\n",
    "\n",
    "parameter_count = 0\n",
    "for name, module in capacity_bandwidth_compositionality.named_modules():\n",
    "    if name.startswith('sender'):\n",
    "        for params in module.parameters():\n",
    "            parameter_count += params.numel()\n",
    "\n",
    "params, order = (parameter_count, len(str(parameter_count)))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Capacity, Bandwidth, and Compositionality in Emergent Language Learning',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f593d2-15cd-43be-b603-5e2d40a9ef19",
   "metadata": {},
   "source": [
    "## 13. The Grammar of Emergent Languages\n",
    "\n",
    "This paper contains this handy-dandy table:\n",
    "\n",
    "![Table showing parameters of the speaker agents in The Grammar of Emergent Languages](grammar-of-emergent-languages.png \"Parameters table\")\n",
    "\n",
    "Actually, 5 consecutive CNN layers with those parameters is impossible, but luckily we never have to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f750ef3-75ac-4d71-8efa-febd4752ae1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6722000, 7)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GrammarNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(GrammarNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            256, # 30 x 30 image\n",
    "            512\n",
    "        )\n",
    "        self.projector = nn.Linear(512, 10000)\n",
    "\n",
    "    def forward(self, img, wrd):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        return F.softmax(self.projector(self.rnn(x)))\n",
    "\n",
    "class GrammarVisualNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(GrammarVisualNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 3, 2)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 3, 2)\n",
    "        self.conv3 = nn.Conv2d(20, 20, 3, 2)\n",
    "        self.conv4 = nn.Conv2d(20, 20, 3, 2)\n",
    "        self.conv5 = nn.Conv2d(20, 20, 3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "params, order = count_parameters(torch.nn.Sequential(GrammarVisualNet(), GrammarNet()))\n",
    "parameter_sizes.append({\n",
    "    'title': 'The Grammar of Emergent Languages',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0137e9d-ea08-4842-8573-170b02bb5dd2",
   "metadata": {},
   "source": [
    "## 14. Emergent Communication at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba7c1a8c-402c-463d-ba3c-1907efb0f4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328870, 7)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The paper says:\n",
    "\n",
    "The speaker’s network architecture is composed of several components to transform the\n",
    "target image x into a message m = (wt)T −1\n",
    "t=0 :\n",
    "• The encoder f is a fixed Resnet-50 architecture that has been previously trained on Ima-\n",
    "genet with the BYOL algorithm. The resulting embedding f (x) is of size 2048.\n",
    "• The RNN hθ used is an LSTM of hidden size 256. Therefore the core state zt,θ is of size\n",
    "512.\n",
    "• The core-state adapter cθ is a linear layer with input size 2048 and an output size of 512 that\n",
    "allows to transform the embedding f (x) into an appropriate core state z−1,θ = cθ (f (x)).\n",
    "We split z−1,θ into two equal parts to obtain the initial hidden state zh,−1,θ and the initial\n",
    "cell state zc,−1,θ .\n",
    "• The word embedder gθ associates to each discrete symbols in W ∪ {sos} an embedding\n",
    "of size 10.\n",
    "• The value head vθ first selects the hidden part zh,t,θ of the core state zt,θ and then applies\n",
    "a linear layer of output size 1.\n",
    "• The policy head πθ first selects the hidden part zh,t,θ of the core state zt,θ and then applies\n",
    "a linear layer of output size |W| to obtain logit\n",
    "\"\"\"\n",
    "\n",
    "class ScaleNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(ScaleNet, self).__init__()\n",
    "        self.core_state_adapter = nn.Linear(2048, 512)\n",
    "        self.word_embedder = nn.Linear(20, 10)\n",
    "        self.rnn = nn.LSTM(10, 256)\n",
    "        self.value_head = nn.Linear(256, 10)\n",
    "        self.policy_head = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, img, wrd):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        x = self.core_state_adapter(x)\n",
    "        return x\n",
    "\n",
    "params, order = count_parameters(ScaleNet())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergent Communication at Scale',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fb485-b3e9-4b87-9a29-016307c50ef0",
   "metadata": {},
   "source": [
    "## 15. Interaction history as a source of compositionality in emergent communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e5eb0-5e8c-42ce-ae05-d8ce5034c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Interaction-history-as-a-source-of-compositionality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efb6de-75b8-40d5-bf21-9f5a41f5fffe",
   "metadata": {},
   "source": [
    "## 16. Emergent communication of generalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241edf6-e1ed-4a4c-8aab-4519f0c9bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('emergent-generalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6607ab-9903-4744-8cfa-39091d586529",
   "metadata": {},
   "source": [
    "## 17.Compositionality Through Language Transmission, using Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc74b30-c780-4591-b664-c49809f1483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('neural-ilm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9a04a-3b78-48ff-8447-6eaf387c98e5",
   "metadata": {},
   "source": [
    "## 18. TexRel: a Green Family of Datasets for Emergent Communications on Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9869c-aef0-4698-9413-49b1b27be1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('texrel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a398d-f4c0-471c-986e-97982830fde5",
   "metadata": {},
   "source": [
    "## 19. Disentangling Categorization in Multi-agent Emergent Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18ce39-6012-4b25-858c-593e7bad7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('disentangling_categorization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43e4ba-52d6-4fd9-8c83-e362c48c4faa",
   "metadata": {},
   "source": [
    "## 20. Emergence of hierarchical reference systems in multi-agent communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccf555-743b-44b5-8be9-df89600271e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('hierarchical_reference_game')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e1d41-c31a-426e-8ea8-a1870b94ab52",
   "metadata": {},
   "source": [
    "## 21. Emergent Communication: Generalization and Overfitting in Lewis Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3852c-c293-4a94-9985-f910f8f43fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Population')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e9035-cc43-450b-a8c5-ab446f34b4f8",
   "metadata": {},
   "source": [
    "## 22. On the Correspondence between Compositionality and Imitation in Emergent Neural Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fff044a-28df-4aad-af7a-2759fd3296e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106880, 6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The paper (https://aclanthology.org/2023.findings-acl.787.pdf) \n",
    "says \"The Sender is a single-layer GRU (Cho et al., 2014)\n",
    "containing a fully-connected (FC) layer that maps the input\n",
    "x to its first hidden state (dim=128).\"\n",
    "And also says\n",
    "\"Each input x denotes an object in an “attribute-\n",
    "value world\", where the object has n_{att} attributes,\n",
    "and each attribute takes n_{val} possible values. We\n",
    "represent x by a concatenation of n_{att} one-hot vec-\n",
    "tors, each of dimension {n_val}.\n",
    "...\n",
    "We set n_{att} = 6, n_{val} = 10\"\n",
    "'''\n",
    "class ImitNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(ImitNet, self).__init__()\n",
    "        self.fc = nn.Linear(6 * 10, 128)\n",
    "        self.gru = nn.GRU(128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "       \n",
    "        return self.gru(self.fc(x))\n",
    "\n",
    "params, order = count_parameters(ImitNet())\n",
    "parameter_sizes.append({\n",
    "    'title': 'On the Correspondence between Compositionality and Imitation in Emergent Neural Communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c21b7-5e96-4547-bdd2-7b2aa877169f",
   "metadata": {},
   "source": [
    "## 23. Compositionality with Variation Reliably Emerges in Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959397b-70eb-40f6-8a08-598753cb4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('variable_compositionality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255da8c-c416-4a46-ab6c-9fc681cb2599",
   "metadata": {},
   "source": [
    "## 24. On the Word Boundaries of Emergent Languages Based on Harris's Articulation Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c564fd43-d62f-455c-9139-f23683bb8527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153600, 7)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inherits_from = 'Compositionality and Generalization in Emergent Languages'\n",
    "previous_calculation = [p for p in parameter_sizes if p['title'] == inherits_from][0]\n",
    "params, order = previous_calculation['params'], previous_calculation['order']\n",
    "\n",
    "parameter_sizes.append({\n",
    "    'title': \"On the Word Boundaries of Emergent Languages Based on Harris's Articulation Scheme\",\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d5a39-901c-427e-a587-e22a46fec3d8",
   "metadata": {},
   "source": [
    "## 25. Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "324c8637-1048-419d-b751-ef2cee667cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2307200, 7)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params, order = count_parameters(nn.Sequential(CompGenNet(), CompGenNet()))\n",
    "parameter_sizes.append({\n",
    "    'title': \"Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments\",\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
