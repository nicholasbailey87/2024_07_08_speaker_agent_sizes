{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb6ba8c-3b92-4cc3-a250-1f467bc17ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"conda_requirements.txt\", \"w\") as crf:\n",
    "    crf.truncate(0)\n",
    "    with open(\"pip_requirements.txt\", \"w\") as prf:\n",
    "        prf.truncate(0)\n",
    "        with open('all_requirements.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                pypi_check = re.search(r'(.+)=pypi.*', line)\n",
    "                if pypi_check:\n",
    "                    print(re.sub(r'=', '==', pypi_check.group(1)).strip(), file=prf)\n",
    "                else:\n",
    "                    print(line.strip(), file=crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db82eb6a-2770-4ea5-a368-4a305c0fdc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - pytorch\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\nicho\\.conda\\envs\\agent_sizes\n",
      "\n",
      "  added / updated specs:\n",
      "    - anyio==4.4.0=pyhd8ed1ab_0\n",
      "    - argon2-cffi-bindings==21.2.0=py38h91455d4_4\n",
      "    - argon2-cffi==23.1.0=pyhd8ed1ab_0\n",
      "    - arrow==1.3.0=pyhd8ed1ab_0\n",
      "    - asttokens==2.4.1=pyhd8ed1ab_0\n",
      "    - async-lru==2.0.4=pyhd8ed1ab_0\n",
      "    - attrs==23.2.0=pyh71513ae_0\n",
      "    - babel==2.14.0=pyhd8ed1ab_0\n",
      "    - backcall==0.2.0=pyh9f0ad1d_0\n",
      "    - beautifulsoup4==4.12.3=pyha770c72_0\n",
      "    - blas==1.0=mkl\n",
      "    - bleach==6.1.0=pyhd8ed1ab_0\n",
      "    - brotli-python==1.1.0=py38hd3f51b4_1\n",
      "    - ca-certificates==2024.7.4=h56e8100_0\n",
      "    - cached-property==1.5.2=hd8ed1ab_1\n",
      "    - cached_property==1.5.2=pyha770c72_1\n",
      "    - certifi==2024.7.4=py38haa95532_0\n",
      "    - cffi==1.16.0=py38h91455d4_0\n",
      "    - charset-normalizer==3.3.2=pyhd8ed1ab_0\n",
      "    - cloudpickle==2.2.1=py38haa95532_0\n",
      "    - colorama==0.4.6=pyhd8ed1ab_0\n",
      "    - comm==0.2.2=pyhd8ed1ab_0\n",
      "    - cuda-cccl==12.5.39=0\n",
      "    - cuda-cccl_win-64==12.5.39=0\n",
      "    - cuda-cudart-dev==12.1.105=0\n",
      "    - cuda-cudart==12.1.105=0\n",
      "    - cuda-cupti==12.1.105=0\n",
      "    - cuda-libraries-dev==12.1.0=0\n",
      "    - cuda-libraries==12.1.0=0\n",
      "    - cuda-nvrtc-dev==12.1.105=0\n",
      "    - cuda-nvrtc==12.1.105=0\n",
      "    - cuda-nvtx==12.1.105=0\n",
      "    - cuda-opencl-dev==12.5.39=0\n",
      "    - cuda-opencl==12.5.39=0\n",
      "    - cuda-profiler-api==12.5.39=0\n",
      "    - cuda-runtime==12.1.0=0\n",
      "    - cuda-version==12.5=3\n",
      "    - debugpy==1.8.2=py38h2698bfa_0\n",
      "    - decorator==5.1.1=pyhd8ed1ab_0\n",
      "    - defusedxml==0.7.1=pyhd8ed1ab_0\n",
      "    - entrypoints==0.4=pyhd8ed1ab_0\n",
      "    - exceptiongroup==1.2.2=pyhd8ed1ab_0\n",
      "    - executing==2.0.1=pyhd8ed1ab_0\n",
      "    - filelock==3.13.1=py38haa95532_0\n",
      "    - fqdn==1.5.1=pyhd8ed1ab_0\n",
      "    - freetype==2.12.1=hdaf720e_2\n",
      "    - gmpy2==2.1.2=py38h7f96b67_0\n",
      "    - gym-notices==0.0.8=pyhd8ed1ab_0\n",
      "    - gym==0.26.1=py38h23ba278_0\n",
      "    - h11==0.14.0=pyhd8ed1ab_0\n",
      "    - h2==4.1.0=pyhd8ed1ab_0\n",
      "    - hpack==4.0.0=pyh9f0ad1d_0\n",
      "    - httpcore==1.0.5=pyhd8ed1ab_0\n",
      "    - httpx==0.27.0=pyhd8ed1ab_0\n",
      "    - hyperframe==6.0.1=pyhd8ed1ab_0\n",
      "    - idna==3.7=pyhd8ed1ab_0\n",
      "    - importlib-metadata==8.0.0=pyha770c72_0\n",
      "    - importlib_metadata==8.0.0=hd8ed1ab_0\n",
      "    - importlib_resources==6.4.0=pyhd8ed1ab_0\n",
      "    - ipykernel==6.29.5=pyh4bbf305_0\n",
      "    - ipython==8.12.2=pyh08f2357_0\n",
      "    - ipywidgets==8.1.3=pyhd8ed1ab_0\n",
      "    - isoduration==20.11.0=pyhd8ed1ab_0\n",
      "    - jedi==0.19.1=pyhd8ed1ab_0\n",
      "    - jinja2==3.1.4=pyhd8ed1ab_0\n",
      "    - json5==0.9.25=pyhd8ed1ab_0\n",
      "    - jsonpointer==3.0.0=py38haa244fe_0\n",
      "    - jsonschema-specifications==2023.12.1=pyhd8ed1ab_0\n",
      "    - jsonschema-with-format-nongpl==4.23.0=hd8ed1ab_0\n",
      "    - jsonschema==4.23.0=pyhd8ed1ab_0\n",
      "    - jupyter-lsp==2.2.5=pyhd8ed1ab_0\n",
      "    - jupyter_client==8.6.2=pyhd8ed1ab_0\n",
      "    - jupyter_core==5.7.2=py38haa244fe_0\n",
      "    - jupyter_events==0.10.0=pyhd8ed1ab_0\n",
      "    - jupyter_server==2.14.2=pyhd8ed1ab_0\n",
      "    - jupyter_server_terminals==0.5.3=pyhd8ed1ab_0\n",
      "    - jupyterlab==4.2.4=pyhd8ed1ab_0\n",
      "    - jupyterlab_pygments==0.3.0=pyhd8ed1ab_1\n",
      "    - jupyterlab_server==2.27.3=pyhd8ed1ab_0\n",
      "    - jupyterlab_widgets==3.0.11=pyhd8ed1ab_0\n",
      "    - krb5==1.21.3=hdf4eb48_0\n",
      "    - lcms2==2.16=h67d730c_0\n",
      "    - lerc==4.0.0=h63175ca_0\n",
      "    - libcublas-dev==12.1.0.26=0\n",
      "    - libcublas==12.1.0.26=0\n",
      "    - libcufft-dev==11.0.2.4=0\n",
      "    - libcufft==11.0.2.4=0\n",
      "    - libcurand-dev==10.3.6.82=0\n",
      "    - libcurand==10.3.6.82=0\n",
      "    - libcusolver-dev==11.4.4.55=0\n",
      "    - libcusolver==11.4.4.55=0\n",
      "    - libcusparse-dev==12.0.2.55=0\n",
      "    - libcusparse==12.0.2.55=0\n",
      "    - libdeflate==1.20=hcfcfb64_0\n",
      "    - libffi==3.4.4=hd77b12b_1\n",
      "    - libhwloc==2.11.1=default_h8125262_1000\n",
      "    - libiconv==1.17=hcfcfb64_2\n",
      "    - libjpeg-turbo==3.0.3=h827c3e9_0\n",
      "    - libnpp-dev==12.0.2.50=0\n",
      "    - libnpp==12.0.2.50=0\n",
      "    - libnvjitlink-dev==12.1.105=0\n",
      "    - libnvjitlink==12.1.105=0\n",
      "    - libnvjpeg-dev==12.1.1.14=0\n",
      "    - libnvjpeg==12.1.1.14=0\n",
      "    - libpng==1.6.43=h19919ed_0\n",
      "    - libsodium==1.0.18=h8d14728_1\n",
      "    - libtiff==4.6.0=hddb2be6_3\n",
      "    - libuv==1.48.0=h827c3e9_0\n",
      "    - libwebp-base==1.4.0=hcfcfb64_0\n",
      "    - libxcb==1.16=hcd874cb_0\n",
      "    - libxml2==2.12.7=h0f24e4e_4\n",
      "    - libzlib==1.3.1=h2466b09_1\n",
      "    - m2w64-gcc-libgfortran==5.3.0=6\n",
      "    - m2w64-gcc-libs-core==5.3.0=7\n",
      "    - m2w64-gcc-libs==5.3.0=7\n",
      "    - m2w64-gmp==6.1.0=2\n",
      "    - m2w64-libwinpthread-git==5.0.0.4634.697f757=2\n",
      "    - markupsafe==2.1.5=py38h91455d4_0\n",
      "    - matplotlib-inline==0.1.7=pyhd8ed1ab_0\n",
      "    - mistune==3.0.2=pyhd8ed1ab_0\n",
      "    - mkl-service==2.4.0=py38h2bbff1b_0\n",
      "    - mkl_fft==1.3.1=py38h277e83a_0\n",
      "    - mkl_random==1.2.2=py38hf11a4ad_0\n",
      "    - mpc==1.1.0=h7edee0f_1\n",
      "    - mpfr==4.0.2=h62dcd97_1\n",
      "    - mpir==3.0.0=hec2e145_1\n",
      "    - mpmath==1.3.0=py38haa95532_0\n",
      "    - msys2-conda-epoch==20160418=1\n",
      "    - nbclient==0.10.0=pyhd8ed1ab_0\n",
      "    - nbconvert-core==7.16.4=pyhd8ed1ab_1\n",
      "    - nbformat==5.10.4=pyhd8ed1ab_0\n",
      "    - nest-asyncio==1.6.0=pyhd8ed1ab_0\n",
      "    - networkx==3.1=py38haa95532_0\n",
      "    - notebook-shim==0.2.4=pyhd8ed1ab_0\n",
      "    - numpy-base==1.24.3=py38h005ec55_0\n",
      "    - numpy==1.24.3=py38hf95b240_0\n",
      "    - openjpeg==2.5.2=h3d672ee_0\n",
      "    - openssl==3.3.1=h2466b09_2\n",
      "    - overrides==7.7.0=pyhd8ed1ab_0\n",
      "    - packaging==24.1=pyhd8ed1ab_0\n",
      "    - pandocfilters==1.5.0=pyhd8ed1ab_0\n",
      "    - parso==0.8.4=pyhd8ed1ab_0\n",
      "    - pickleshare==0.7.5=py_1003\n",
      "    - pillow==10.4.0=py38h8d10685_0\n",
      "    - pip==24.0=py38haa95532_0\n",
      "    - pkgutil-resolve-name==1.3.10=pyhd8ed1ab_1\n",
      "    - platformdirs==4.2.2=pyhd8ed1ab_0\n",
      "    - prometheus_client==0.20.0=pyhd8ed1ab_0\n",
      "    - prompt-toolkit==3.0.47=pyha770c72_0\n",
      "    - prompt_toolkit==3.0.47=hd8ed1ab_0\n",
      "    - psutil==6.0.0=py38h4cb3324_0\n",
      "    - pthread-stubs==0.3=h3c9f919_1\n",
      "    - pthreads-win32==2.9.1=hfa6e2cd_3\n",
      "    - pure_eval==0.2.2=pyhd8ed1ab_0\n",
      "    - pycparser==2.22=pyhd8ed1ab_0\n",
      "    - pygments==2.18.0=pyhd8ed1ab_0\n",
      "    - pysocks==1.7.1=pyh0701188_6\n",
      "    - python-dateutil==2.9.0=pyhd8ed1ab_0\n",
      "    - python-fastjsonschema==2.20.0=pyhd8ed1ab_0\n",
      "    - python-json-logger==2.0.7=pyhd8ed1ab_0\n",
      "    - python==3.8.19=h1aa4202_0\n",
      "    - python_abi==3.8=2_cp38\n",
      "    - pytorch-cuda==12.1=hde6ce7c_5\n",
      "    - pytorch-mutex==1.0=cuda\n",
      "    - pytorch==2.3.1=py3.8_cuda12.1_cudnn8_0\n",
      "    - pytz==2024.1=pyhd8ed1ab_0\n",
      "    - pywin32==306=py38hd3f51b4_2\n",
      "    - pywinpty==2.0.13=py38hd3f51b4_0\n",
      "    - pyyaml==6.0.1=py38h91455d4_1\n",
      "    - pyzmq==26.0.3=py38h3c56b06_0\n",
      "    - referencing==0.35.1=pyhd8ed1ab_0\n",
      "    - requests==2.32.3=pyhd8ed1ab_0\n",
      "    - rfc3339-validator==0.1.4=pyhd8ed1ab_0\n",
      "    - rfc3986-validator==0.1.1=pyh9f0ad1d_0\n",
      "    - rpds-py==0.19.0=py38h2e0ef18_0\n",
      "    - send2trash==1.8.3=pyh5737063_0\n",
      "    - setuptools==69.5.1=py38haa95532_0\n",
      "    - six==1.16.0=pyh6c4a22f_0\n",
      "    - sniffio==1.3.1=pyhd8ed1ab_0\n",
      "    - soupsieve==2.5=pyhd8ed1ab_1\n",
      "    - sqlite==3.45.3=h2bbff1b_0\n",
      "    - stack_data==0.6.2=pyhd8ed1ab_0\n",
      "    - sympy==1.12=py38haa95532_0\n",
      "    - terminado==0.18.1=pyh5737063_0\n",
      "    - tinycss2==1.3.0=pyhd8ed1ab_0\n",
      "    - tk==8.6.13=h5226925_1\n",
      "    - tomli==2.0.1=pyhd8ed1ab_0\n",
      "    - tornado==6.4.1=py38h4cb3324_0\n",
      "    - traitlets==5.14.3=pyhd8ed1ab_0\n",
      "    - types-python-dateutil==2.9.0.20240316=pyhd8ed1ab_0\n",
      "    - typing-extensions==4.12.2=hd8ed1ab_0\n",
      "    - typing_extensions==4.12.2=pyha770c72_0\n",
      "    - typing_utils==0.1.0=pyhd8ed1ab_0\n",
      "    - ucrt==10.0.22621.0=h57928b3_0\n",
      "    - uri-template==1.3.0=pyhd8ed1ab_0\n",
      "    - urllib3==2.2.2=pyhd8ed1ab_1\n",
      "    - vc14_runtime==14.40.33810=ha82c5b3_20\n",
      "    - vc==14.2=h2eaa2aa_4\n",
      "    - vs2015_runtime==14.40.33810=h3bf8584_20\n",
      "    - wcwidth==0.2.13=pyhd8ed1ab_0\n",
      "    - webcolors==24.6.0=pyhd8ed1ab_0\n",
      "    - webencodings==0.5.1=pyhd8ed1ab_2\n",
      "    - websocket-client==1.8.0=pyhd8ed1ab_0\n",
      "    - wheel==0.43.0=py38haa95532_0\n",
      "    - widgetsnbextension==4.0.11=pyhd8ed1ab_0\n",
      "    - win_inet_pton==1.1.0=pyhd8ed1ab_6\n",
      "    - winpty==0.4.3=4\n",
      "    - xorg-libxau==1.0.11=hcd874cb_0\n",
      "    - xorg-libxdmcp==1.1.3=hcd874cb_0\n",
      "    - xz==5.4.6=h8cc25b3_1\n",
      "    - yaml==0.2.5=h8ffe710_2\n",
      "    - zeromq==4.3.5=he1f189c_4\n",
      "    - zipp==3.19.2=pyhd8ed1ab_0\n",
      "    - zstandard==0.23.0=py38hf92978b_0\n",
      "    - zstd==1.5.6=h0ea2cb4_0\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    blas-1.0                   |              mkl           6 KB\n",
      "    m2w64-gcc-libgfortran-5.3.0|                6         340 KB\n",
      "    m2w64-gcc-libs-5.3.0       |                7         518 KB\n",
      "    m2w64-gcc-libs-core-5.3.0  |                7         213 KB\n",
      "    m2w64-gmp-6.1.0            |                2         689 KB\n",
      "    m2w64-libwinpthread-git-5.0.0.4634.697f757|                2          30 KB\n",
      "    mpfr-4.0.2                 |       h62dcd97_1         1.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.3 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  blas                                          conda-forge --> pkgs/main \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  m2w64-gcc-libgfor~                            conda-forge --> pkgs/msys2 \n",
      "  m2w64-gcc-libs                                conda-forge --> pkgs/msys2 \n",
      "  m2w64-gcc-libs-co~                            conda-forge --> pkgs/msys2 \n",
      "  m2w64-gmp                                     conda-forge --> pkgs/msys2 \n",
      "  m2w64-libwinpthre~                            conda-forge --> pkgs/msys2 \n",
      "  mpfr                                          conda-forge --> pkgs/main \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y --file conda_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0f7251-67f4-4647-ac79-85c6778ecb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -r pip_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc02874-0fa9-4a37-ac53-a0a07bb528be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import pdb\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb856e76-1074-406f-99df-535a9bcea753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\git_repos\\\\2024_05_12_speaker_agent_sizes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "original_working_directory = os.getcwd()\n",
    "original_working_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3c55ce-6af9-4c9b-ac2d-304a92c2c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(module):\n",
    "    parameter_count = 0\n",
    "    for params in module.parameters():\n",
    "        parameter_count += params.numel()\n",
    "    return (parameter_count, len(str(parameter_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cb976b-2283-4ca2-a0b1-e93922fc60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this a few times\n",
    "\n",
    "from transformers import ResNetForImageClassification\n",
    "\n",
    "resnet = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "params_resnet, _ = count_parameters(resnet)\n",
    "\n",
    "del resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee12fde-5f0f-415b-bbd5-3f96dcd48b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_sizes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd363ac-0b21-40b3-a8f3-7cc68ce38321",
   "metadata": {},
   "source": [
    "# 1. Natural language does not emerge ’naturally’ in multi-agent dialog\n",
    "\n",
    "This paper uses symbollic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0be0b82-7524-4b57-8e5b-352e92b8a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "\t  hiddenSize : 50\n",
      "\t   embedSize : 20\n",
      "\t imgFeatSize : 20\n",
      "\t   qOutVocab : 3\n",
      "\t   aOutVocab : 4\n",
      "\t     dataset : data/64_synthetic.json\n",
      "\t     rlScale : 100.0\n",
      "\t   numRounds : 2\n",
      "\t    remember : False\n",
      "\t negFraction : 0.8\n",
      "\t   batchSize : 1000\n",
      "\t   numEpochs : 1000000\n",
      "\tlearningRate : 0.001\n",
      "\t      useGPU : False\n",
      "Answerer(\n",
      "  (inNet): Embedding(7, 20)\n",
      "  (outNet): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (imgNet): Embedding(12, 20)\n",
      "  (rnn): LSTMCell(80, 50)\n",
      ")\n",
      "Questioner(\n",
      "  (inNet): Embedding(16, 20)\n",
      "  (outNet): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (rnn): LSTMCell(20, 50)\n",
      "  (predictRNN): LSTMCell(20, 50)\n",
      "  (predictNet): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29885, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('lang-emerge')\n",
    "\n",
    "# Based on what is in train.py!\n",
    "\n",
    "from chatbots import Team\n",
    "from dataloader import Dataloader\n",
    "import options # sets the default hyperparameters etc.\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# read the command line options\n",
    "options = options.read()\n",
    "#------------------------------------------------------------------------\n",
    "# setup experiment and dataset\n",
    "#------------------------------------------------------------------------\n",
    "data = Dataloader(options)\n",
    "numInst = data.getInstCount()\n",
    "\n",
    "params = data.params\n",
    "# append options from options to params\n",
    "for key, value in options.items():\n",
    "  params[key] = value\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# build agents, and setup optmizer\n",
    "#------------------------------------------------------------------------\n",
    "team = Team(params)\n",
    "\n",
    "speaker = team.qBot\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Natural language does not emerge ’naturally’ in multi-agent dialog',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece762b-3d72-4b4b-8410-106a75303878",
   "metadata": {},
   "source": [
    "## 2. Emergence of Grounded Compositional Language in Multi-Agent Populations\n",
    "\n",
    "This paper uses symbollic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e940afa-0728-4935-8fa9-fde5fec80bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1870105, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('emergent-language/modules')\n",
    "\n",
    "from agent import AgentModule\n",
    "from configs import default_agent_config\n",
    "\n",
    "speaker = AgentModule(default_agent_config)\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergence of Grounded Compositional Language in Multi-Agent Populations',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02125c-fac8-43a6-b4e9-3483aee316d8",
   "metadata": {},
   "source": [
    "## 3. Emergence of Communication in an Interactive World with Consistent Speakers\n",
    "\n",
    "This speaker agent uses images as input and the CNN is initialised as part of the speaker agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75478047-c791-49e5-aac6-83588a461779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45253, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('emergence-communication-cco/agent')\n",
    "\n",
    "import agent_type\n",
    "from vec_agent import VecAgent\n",
    "\n",
    "speaker = VecAgent(\n",
    "    agent_type.AgentType.speaker,\n",
    "    acting=True # this parameter does not affect the speaker agent size\n",
    ")\n",
    "\n",
    "params, order = count_parameters(speaker.model)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergence of Communication in an Interactive World with Consistent Speakers',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64496e-878b-4b0e-bd02-a71e737bb73a",
   "metadata": {},
   "source": [
    "## 4. Compositional Obverter Communication Learning From Raw Visual Input\n",
    "\n",
    "This speaker agent uses images as input and the CNN is initialised as part of the speaker agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c156593c-a89e-4c07-b055-2dc5cc7f3f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64676, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('obverter')\n",
    "\n",
    "from model import ConvModel # Not just a CNN but an agent\n",
    "\n",
    "speaker = ConvModel(\n",
    "    5 # vocabulary size, taken from the paper\n",
    ")\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositional Obverter Communication Learning From Raw Visual Input',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28743794-029a-4269-93aa-1f4a13641e87",
   "metadata": {},
   "source": [
    "## 5. Emergence of Compositional Language with Deep Generational Transmission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b18d0f-2ff1-43fe-9a86-8b02ece38d4f",
   "metadata": {},
   "source": [
    "We have to temporarily install parlai for this one, then uninstall it, then clean up after ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dda0921-c4c2-4b9c-b031-9ad19fc81ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jsonschema 4.23.0 requires attrs>=22.2.0, but you have attrs 20.2.0 which is incompatible.\n",
      "referencing 0.35.1 requires attrs>=22.2.0, but you have attrs 20.2.0 which is incompatible.\n",
      "transformers 4.42.4 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.17.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install parlai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01fbc68f-b6f5-493c-a0d7-14c4aa5c1588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107956, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('evolang')\n",
    "\n",
    "from bots import Questioner\n",
    "\n",
    "params, order = count_parameters(Questioner(\n",
    "    { \n",
    "        'q_out_vocab': 64, # Max seen in run.py\n",
    "        'a_out_vocab': 64, # Max seen in run.py\n",
    "        'task_vocab': 6, # Defined in train.py as len(dataset.task_defn) which based on the datasets folder is typically 6\n",
    "        'embed_size': 20, # default from options.py\n",
    "        'hidden_size': 100, # default from options.py\n",
    "        'props': {'shapes': [0, 1, 2, 3], 'styles': [0, 1, 2, 3], 'colors': [0, 1, 2, 3]} # From the json in the datasets folder\n",
    "    }\n",
    "))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergence of Compositional Language with Deep Generational Transmission',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8c75f-a3e7-4e40-9b95-56c9f3bcc0f9",
   "metadata": {},
   "source": [
    "Reinstall newer version of attrs so that everything else can run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8de8fa9-87a8-409e-b9ca-87a957aadf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y parlai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ec3369-48dc-4a7c-8c9a-ea2d884bf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe54e77-00e7-4ec2-8963-a87bcbf10ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - pytorch\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -y --file conda_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82e59a4c-852a-4e3c-97cf-74a834d62b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -r pip_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f44e2-698c-4188-b75a-49635800ea39",
   "metadata": {},
   "source": [
    "## 6. The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5397d8-4dc0-4390-8fbe-2446518134cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6264858, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The paper says they use an LSTM speaker like Havrylov and Titov (2017) \"Emergence of language with multi-agent game...\"\n",
    "# Havrylov and Titov say they use an LSTM with hidden size 512 and vocabulary 10000\n",
    "# And also a LeNet for embedding images\n",
    "\n",
    "class HavrylovNet(nn.Module):\n",
    "    # Based on https://github.com/lychengrex/LeNet-5-Implementation-Using-Pytorch/blob/master/LeNet-5%20Implementation%20Using%20Pytorch.ipynb\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(HavrylovNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            10, # LSTM input size is LeNet output size, i.e. 10\n",
    "            512\n",
    "        )\n",
    "        self.projector = nn.Linear(512, 10000)\n",
    "\n",
    "    def forward(self, img, wrd):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        return F.softmax(self.projector(self.rnn(x)))\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        '''\n",
    "        Get the number of features in a batch of tensors `x`.\n",
    "        '''\n",
    "        size = x.size()[1:]\n",
    "        return np.prod(size)\n",
    "\n",
    "params, order = count_parameters(torch.nn.Sequential(LeNet(), HavrylovNet()))\n",
    "parameter_sizes.append({\n",
    "    'title': 'The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fa5a3-83c6-40e6-a179-9d7c10a075a9",
   "metadata": {},
   "source": [
    "## 7. Ease-of-Teaching and Language Structure from Emergent Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4396d032-3a2d-48e0-a0ba-2c5f841fb39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46108, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Ease-of-teaching-and-language-structure/code')\n",
    "\n",
    "import argparse\n",
    "# The initial arguments from parse are copied from parse.py\n",
    "def parse():\n",
    "    parser = argparse.ArgumentParser(description='Referential game settings')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='which gpu if we use gpu')\n",
    "    parser.add_argument('--fname', type=str, default='test', help='folder name to save results')\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--jupyter', action='store_true') \n",
    "    parser.add_argument('--slambda', type=float, default=0.1, help='speaker regularization hyperparameter')\n",
    "    parser.add_argument('--rlambda', type=float, default=0.1, help='listener regularization hyperparameter')\n",
    "    parser.add_argument('--receiverNum', type=int, default=1, help='number of listeners in the population')\n",
    "    parser.add_argument('--topk', type=int, default=3, help='number of top messages when we probe language')\n",
    "    parser.add_argument('--evaluateSize', type=int, default=1000, help='the batch size of test objects when not enumeration')\n",
    "    args_dict = vars(parser.parse_args([])) # convert python object to dict\n",
    "    return args_dict\n",
    "\n",
    "args = parse()  # parsed argument from CLI\n",
    "args['device'] = torch.device(\"cuda:\" + str(args['gpu']) if torch.cuda.is_available() else \"cpu\")\n",
    "if not os.path.exists(args['fname']):\n",
    "    os.makedirs(args['fname'])\n",
    "\n",
    "# dataset hyperparameters\n",
    "args['numColors'] = 8 # based on the paper https://arxiv.org/pdf/1906.02403\n",
    "args['numShapes'] = 4  # based on the paper https://arxiv.org/pdf/1906.02403\n",
    "args['attrSize'] = args['numColors'] + args['numShapes']  # colors + shapes\n",
    "\n",
    "# game settings\n",
    "args['vocabSize'] = 8  # based on the paper https://arxiv.org/pdf/1906.02403\n",
    "args['messageLen'] = 2  # based on the paper https://arxiv.org/pdf/1906.02403\n",
    "args['distractNum'] = 5  # including targets\n",
    "\n",
    "# training hyperparameters\n",
    "args['batchSize'] = 100  # total train data = batchSize * numIters\n",
    "args['sLearnRate'] = 0.001  \n",
    "args['rLearnRate'] = 0.001  \n",
    "\n",
    "args['trainIters'] = 300000 # training\n",
    "args['resetNum'] = 50  \n",
    "args['resetIter'] = args['trainIters'] // args['resetNum']  # life of a receiver: 6K\n",
    "args['deterResetNums'] = 30\n",
    "args['deterResetIter'] = 1000\n",
    "\n",
    "# population of receivers training\n",
    "args['population'] = False\n",
    "\n",
    "# model hyperparameters\n",
    "args['hiddenSize'] = 100  # based on the paper https://arxiv.org/pdf/1906.02403\n",
    "\n",
    "from models import Sender\n",
    "\n",
    "params, order = count_parameters(Sender(args))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Ease-of-Teaching and Language Structure from Emergent Communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a2a65-41f6-40d1-b78e-0e15e45e1aed",
   "metadata": {},
   "source": [
    "## 8. Compositional Languages Emerge in a Neural Iterated Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6172b5d-e6bd-45db-83a5-9772fd44aae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73872, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Neural_Iterated_Learning/models')\n",
    "\n",
    "if \"model\" in sys.modules:\n",
    "    del sys.modules[\"model\"] # we used model before from a different repository!\n",
    "from model import SpeakingAgent\n",
    "\n",
    "os.chdir('../utils')\n",
    "import conf\n",
    "\n",
    "params, order = count_parameters(SpeakingAgent())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositional Languages Emerge in a Neural Iterated Learning Model',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a2854-4372-4311-a565-9f8973e34ee5",
   "metadata": {},
   "source": [
    "## 9. Compositionality and Generalization in Emergent Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05c8e3a5-748b-4f0c-905b-fb58f44ca8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153600, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The paper says\n",
    "\n",
    "  Each input i of the reconstruction game is comprised of iatt attributes,\n",
    "  each with ival possible values. We let i_{att} range from 2 to 4 and i_{val}\n",
    "  from 4 to 100. We represent each attribute as a i_{val} one-hot vector. An\n",
    "  input i is given by the concatenation of its attributes. \n",
    "\n",
    "  ...\n",
    "\n",
    "  Both agents are implemented as single-layer GRU cells (Cho et al., 2014)\n",
    "  with hidden states of size 500.\n",
    "  \n",
    "  Sender encodes i in a message m of fixed length c_{len} as follows. First,\n",
    "  a linear layer maps the input vector into the initial hidden state of\n",
    "  Sender. Next, the message is generated symbol-by-symbol by sampling from a\n",
    "  Categorical distribution over the vocabulary cvoc, parameterized by a linear\n",
    "  mapping from Sender’s hidden state.\n",
    "\n",
    "  In practice, we fix [vocabulary size to 100].\n",
    "\"\"\"\n",
    "\n",
    "class CompGenNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(CompGenNet, self).__init__()\n",
    "        self.fc1   = nn.Linear(400, 500)\n",
    "        self.gru   = nn.GRU(100, 500)\n",
    "        self.fc2   = nn.Linear(500, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        hidden = self.fc1(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        return self.fc2(out)\n",
    "\n",
    "params, order = count_parameters(CompGenNet())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositionality and Generalization in Emergent Languages',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e4912-9131-49ed-b894-ad815b86623a",
   "metadata": {},
   "source": [
    "## 10. Co-evolution of language and agents in referential games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af5969c6-3c98-4d07-b799-33ebd8cda360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1696909, 7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('cultural-evolution-engine/model')\n",
    "\n",
    "from ShapesModels import ShapesSender\n",
    "from visual_module import CNN\n",
    "\n",
    "cnn = CNN(\n",
    "    # paper says \"The linear layer which followed the convolutional layers had output dimensions of 512\"\n",
    "    n_out_features=512\n",
    ")\n",
    "\n",
    "sender = ShapesSender(vocab_size=5, output_len=5, sos_id=0)\n",
    "params, order = count_parameters(torch.nn.Sequential(cnn, sender))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Co-evolution of language and agents in referential games',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c347d-0c99-416f-8d4e-35d54247c1a2",
   "metadata": {},
   "source": [
    "## 11. Inductive Bias and Language Expressivity in Emergent Communication\n",
    "\n",
    "This repository requires us to install a package from a git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a356a2b5-4df0-40e6-873a-ef6f0e76d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/EGG.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0482c667-2029-4171-a4fe-88a6f11ac3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\.conda\\envs\\agent_sizes\\lib\\site-packages\\egg\\core\\language_analysis.py:11: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n",
      "  import numpy as np\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lapack_lite: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DspritesSenderCNN\n\u001b[0;32m      9\u001b[0m params, order \u001b[38;5;241m=\u001b[39m count_parameters(DspritesSenderCNN())\n\u001b[0;32m     10\u001b[0m parameter_sizes\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInductive Bias and Language Expressivity in Emergent Communication\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m: order\n\u001b[0;32m     14\u001b[0m })\n",
      "File \u001b[1;32mC:\\git_repos\\2024_05_12_speaker_agent_sizes\\GameBias-EmeCom2020\\modules.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01minit\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01megg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseGame\u001b[39;00m(metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n\u001b[0;32m      9\u001b[0m     hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\agent_sizes\\lib\\site-packages\\egg\\core\\__init__.py:31\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgs_wrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     GumbelSoftmaxWrapper,\n\u001b[0;32m     23\u001b[0m     RelaxedEmbedding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     SymbolReceiverWrapper,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interaction, LoggingStrategy, dump_interactions\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     Disent,\n\u001b[0;32m     33\u001b[0m     MessageEntropy,\n\u001b[0;32m     34\u001b[0m     PrintValidationEvents,\n\u001b[0;32m     35\u001b[0m     TopographicSimilarity,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiscriminationLoss, NTXentLoss, ReconstructionLoss\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullSweepAgentSampler, PopulationGame, UniformAgentSampler\n",
      "File \u001b[1;32m~\\.conda\\envs\\agent_sizes\\lib\\site-packages\\egg\\core\\language_analysis.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spearmanr\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interaction\n",
      "File \u001b[1;32m~\\.conda\\envs\\agent_sizes\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\agent_sizes\\lib\\site-packages\\scipy\\stats\\_stats_py.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n",
      "File \u001b[1;32m~\\.conda\\envs\\agent_sizes\\lib\\site-packages\\numpy\\testing\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestCase\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extbuild, decorators \u001b[38;5;28;01mas\u001b[39;00m dec\n",
      "File \u001b[1;32m~\\.conda\\envs\\agent_sizes\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m(\n\u001b[0;32m     22\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack_lite\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[0;32m     27\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_equal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_almost_equal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_approx_equal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_array_equal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_array_less\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_string_equal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OLD_PROMOTION\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m         ]\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lapack_lite: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('GameBias-EmeCom2020')\n",
    "\n",
    "if \"numpy\" in sys.modules:\n",
    "    del sys.modules[\"numpy\"]\n",
    "\n",
    "from modules import DspritesSenderCNN\n",
    "\n",
    "params, order = count_parameters(DspritesSenderCNN())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Inductive Bias and Language Expressivity in Emergent Communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352296c-37cd-46dd-8ab8-1304a9ae36b0",
   "metadata": {},
   "source": [
    "## 12. Capacity, Bandwidth, and Compositionality in Emergent Language Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f964bc-cfa4-4842-9331-f19d979430e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "os.chdir(original_working_directory)\n",
    "\n",
    "# Read in the file\n",
    "with open('cbc-emecom/main.py', 'r') as file:\n",
    "  filedata = file.read()\n",
    "\n",
    "# Replace the target string\n",
    "filedata = filedata.replace('parser.add_argument', '#')\n",
    "\n",
    "# Write the file out again\n",
    "with open('cbc-emecom/main.py', 'w') as file:\n",
    "  file.write(filedata)\n",
    "os.chdir(\"cbc-emecom\")\n",
    "\n",
    "# sys.argv = [\"main.py\", \"--num-binary-messages\", \"24\", \"--num-digits\", \"6\", \"--embedding-size-sender\", \"40\", \"--project-size-sender\", \"60\", \"--num-lstm-sender\", \"300\", \"--num-lstm-receiver\", \"325\", \"--embedding-size-receiver\", \"125\", \"--save-str\", \"<SAVE_STR>\"]\n",
    "\n",
    "from main import CompCap\n",
    "\n",
    "config = {\n",
    "    'device': device,\n",
    "    'num_binary_messages': 24,\n",
    "    'seed': 0,\n",
    "    \n",
    "    # problem size\n",
    "    'batch_size': 100,\n",
    "    'num_digits': 6,\n",
    "    'signature_size': 2,\n",
    "    \n",
    "    # network params\n",
    "    'embedding_size_sender': 40,\n",
    "    'project_size_sender': 60,\n",
    "    'num_lstm_sender': 300,\n",
    "    'num_lstm_receiver': 325,\n",
    "    'embedding_size_receiver': 125,\n",
    "    \n",
    "    # optimization params\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'output_loss_penalty': 1,\n",
    "    'weight_norm_penalty': 1e-4,\n",
    "    'temp': 1,\n",
    "    'max_iters': 200000,\n",
    "    'train_acc': 0.60,\n",
    "    'trainval_acc': 0.60,\n",
    "    # logging/printing\n",
    "    'trainval_interval': 50,\n",
    "    'model_dir': None,\n",
    "    'save_str': '',\n",
    "    'log_dir': \"./logs\",\n",
    "    'save_dir': \"./models\"\n",
    "}\n",
    "\n",
    "# parameter_count = 0\n",
    "\n",
    "capacity_bandwidth_compositionality = CompCap(config)\n",
    "\n",
    "parameter_count = 0\n",
    "for name, module in capacity_bandwidth_compositionality.named_modules():\n",
    "    if name.startswith('sender'):\n",
    "        for params in module.parameters():\n",
    "            parameter_count += params.numel()\n",
    "\n",
    "params, order = (parameter_count, len(str(parameter_count)))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Capacity, Bandwidth, and Compositionality in Emergent Language Learning',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0137e9d-ea08-4842-8573-170b02bb5dd2",
   "metadata": {},
   "source": [
    "## 13. Emergent Communication at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c1a8c-402c-463d-ba3c-1907efb0f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ResNetForImageClassification\n",
    "\n",
    "resnet = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "params_resnet, _ = count_parameters(resnet)\n",
    "\n",
    "del resnet\n",
    "\n",
    "\"\"\"\n",
    "The paper says:\n",
    "\n",
    "The speaker’s network architecture is composed of several components to transform the\n",
    "target image x into a message m = (wt)T −1\n",
    "t=0 :\n",
    "• The encoder f is a fixed Resnet-50 architecture that has been previously trained on Ima-\n",
    "genet with the BYOL algorithm. The resulting embedding f (x) is of size 2048.\n",
    "• The RNN hθ used is an LSTM of hidden size 256. Therefore the core state zt,θ is of size\n",
    "512.\n",
    "• The core-state adapter cθ is a linear layer with input size 2048 and an output size of 512 that\n",
    "allows to transform the embedding f (x) into an appropriate core state z−1,θ = cθ (f (x)).\n",
    "We split z−1,θ into two equal parts to obtain the initial hidden state zh,−1,θ and the initial\n",
    "cell state zc,−1,θ .\n",
    "• The word embedder gθ associates to each discrete symbols in W ∪ {sos} an embedding\n",
    "of size 10.\n",
    "• The value head vθ first selects the hidden part zh,t,θ of the core state zt,θ and then applies\n",
    "a linear layer of output size 1.\n",
    "• The policy head πθ first selects the hidden part zh,t,θ of the core state zt,θ and then applies\n",
    "a linear layer of output size |W| to obtain logit\n",
    "\"\"\"\n",
    "\n",
    "class ScaleNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(ScaleNet, self).__init__()\n",
    "        self.core_state_adapter = nn.Linear(2048, 512)\n",
    "        self.word_embedder = nn.Linear(20, 10)\n",
    "        self.rnn = nn.LSTM(10, 256)\n",
    "        self.value_head = nn.Linear(256, 10)\n",
    "        self.policy_head = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, img, wrd):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        x = self.core_state_adapter(x)\n",
    "        return x\n",
    "\n",
    "params, _ = count_parameters(ScaleNet())\n",
    "params += params_resnet\n",
    "order = len(str(params))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergent Communication at Scale',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fb485-b3e9-4b87-9a29-016307c50ef0",
   "metadata": {},
   "source": [
    "## 14. Interaction history as a source of compositionality in emergent communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e5eb0-5e8c-42ce-ae05-d8ce5034c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Interaction-history-as-a-source-of-compositionality/common')\n",
    "\n",
    "from egg import core\n",
    "from visual_classifier import Vision\n",
    "os.chdir('../template_transfer')\n",
    "from agents import Sender\n",
    "\n",
    "cnn = Vision()\n",
    "os.chdir('..')\n",
    "speaker = core.RnnSenderReinforce(\n",
    "    # parameters mostly from defaults in template_transfer/train.py\n",
    "    agent=Sender(200, Vision.from_pretrained('vision_model.pth')),\n",
    "    vocab_size=10, # from paper\n",
    "    embed_dim=50,\n",
    "    hidden_size=200,\n",
    "    max_len=1,\n",
    "    cell='rnn'\n",
    ")\n",
    "\n",
    "params, order = count_parameters(torch.nn.Sequential(cnn, speaker))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Interaction history as a source of compositionality in emergent communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efb6de-75b8-40d5-bf21-9f5a41f5fffe",
   "metadata": {},
   "source": [
    "## 15. Emergent communication of generalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241edf6-e1ed-4a4c-8aab-4519f0c9bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('emergent-generalization/code/models')\n",
    "\n",
    "from speaker import Speaker\n",
    "from vision import ResNet18\n",
    "\n",
    "vocab_size = 20 # vocab for the birds game as specified in the paper\n",
    "embedding_size = 500 # from the paper\n",
    "\n",
    "#This is based on what's in builder.py:\n",
    "speaker = Speaker(ResNet18(), nn.Embedding(vocab_size + 3, embedding_size))\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergent communication of generalizations',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6607ab-9903-4744-8cfa-39091d586529",
   "metadata": {},
   "source": [
    "## 16. Compositionality Through Language Transmission, using Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc74b30-c780-4591-b664-c49809f1483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('neural-ilm/ilm')\n",
    "\n",
    "from ilm_rnn_2018jan import AgentModel\n",
    "\n",
    "speaker = AgentModel(\n",
    "    # based on the defaults in ilm_rnn_2018jan.py\n",
    "    5,\n",
    "    10,\n",
    "    50,\n",
    "    4,\n",
    "    20\n",
    ")\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositionality Through Language Transmission, using Artificial Neural Networks',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9a04a-3b78-48ff-8447-6eaf387c98e5",
   "metadata": {},
   "source": [
    "## 17. TexRel: a Green Family of Datasets for Emergent Communications on Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499f908-735f-4393-ac9b-2429c2db9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('texrel/ref_task/models')\n",
    "\n",
    "if \"models\" in sys.modules:\n",
    "    del sys.modules[\"models\"] # we used models before from a different repository!\n",
    " \n",
    "from sender_model import SenderModel\n",
    "from image_seq_embedders import PrototypicalSender\n",
    "from decoders_differentiable import RNNDecoder\n",
    "from pre_conv import StridedConv\n",
    "\n",
    "speaker = SenderModel(\n",
    "    # Arguments here based on defaults in ref_task/params_groups.py\n",
    "    StridedConv(\n",
    "        3, # Implied to be equal to grid_planes (see below) in ref_task/models/conv_models.py\n",
    "        64, # Implied to be 64 by default in ref_task/models/conv_models.py\n",
    "        0.2, # In ref_task/params_groups.py as --preconv-dropout\n",
    "        4, # In ref_task/params_groups.py as --preconv-stride\n",
    "        True # In ref_task/params_groups.py as --preconv-relu\n",
    "    ),\n",
    "    PrototypicalSender(\n",
    "        128, # In ref_task/params_groups.py as --embedding-size\n",
    "        0, # In ref_task/params_groups.py as --dropout\n",
    "        (16, 16), # In ref_task/params_groups.py as --cnn-sizes, wants to be passed as an iterable here\n",
    "        5 * 5, # In texrel/create_colletion.py as --grid-size, default 5, and needs to be multiplied by the size of the textures in the texrel images as shown in texrel/dataset_runtine.py, which (looking at the images in the paper) is about 5 pixels\n",
    "        3, # In texrel/dataset_runtine.py as self.meta.grid_planes\n",
    "        None, # In ref_task/params_groups.py as --cnn-max-pooling-size\n",
    "        True # In ref_task/params_groups.py as --cnn-batch-norm\n",
    "    ),\n",
    "    RNNDecoder(\n",
    "        128, # In ref_task/params_groups.py as --embedding-size\n",
    "        10, # In ref_task/params_groups.py as --utt-len\n",
    "        21 # In ref_task/params_groups.py as --vocab-size\n",
    "    ),\n",
    "    True\n",
    ")\n",
    "\n",
    "params, order = count_parameters(speaker)\n",
    "parameter_sizes.append({\n",
    "    'title': 'TexRel: a Green Family of Datasets for Emergent Communications on Relations',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a398d-f4c0-471c-986e-97982830fde5",
   "metadata": {},
   "source": [
    "## 18. Disentangling Categorization in Multi-agent Emergent Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18ce39-6012-4b25-858c-593e7bad7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('disentangling_categorization/research_pool/config_archive/SEMIOSIS/111121-065556')\n",
    "\n",
    "import json\n",
    "with open('0_seed-0.json') as config: # This config uses a Resnet-50 as CNN\n",
    "  state = json.loads(config.read())\n",
    "state['device'] = 'cuda'\n",
    "\n",
    "os.chdir(original_working_directory)\n",
    "os.chdir('disentangling_categorization')\n",
    "\n",
    "import os\n",
    "os.environ[\"DATA_ROOT\"] = \"data\"\n",
    "os.environ[\"SAVE_ROOT\"] = \"SAVE_ROOT\"\n",
    "os.environ[\"DISENT_ROOT\"] = \".\"\n",
    "\n",
    "import model_builder\n",
    "\n",
    "speaker = model_builder.build_complete_sender(state)\n",
    "\n",
    "params, order = count_parameters(torch.nn.Sequential(speaker[0], speaker[1]))\n",
    "parameter_sizes.append({\n",
    "    'title': 'Disentangling Categorization in Multi-agent Emergent Communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43e4ba-52d6-4fd9-8c83-e362c48c4faa",
   "metadata": {},
   "source": [
    "## 19. Emergence of hierarchical reference systems in multi-agent communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccf555-743b-44b5-8be9-df89600271e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('hierarchical_reference_game')\n",
    "\n",
    "from archs import Sender\n",
    "\n",
    "# based on what's in train.py\n",
    "sender = Sender(256, 32, 4)\n",
    "\n",
    "params, order = count_parameters(sender)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergence of hierarchical reference systems in multi-agent communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e1d41-c31a-426e-8ea8-a1870b94ab52",
   "metadata": {},
   "source": [
    "## 20. Emergent Communication: Generalization and Overfitting in Lewis Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3852c-c293-4a94-9985-f910f8f43fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(original_working_directory)\n",
    "os.chdir('Population/default_experiment/default_json')\n",
    "\n",
    "import json\n",
    "with open('agents.json') as config:\n",
    "  sender_params = json.loads(config.read())['sender_default_reco']['sender_params']\n",
    "#state['device'] = 'cuda'\n",
    "\n",
    "os.chdir(original_working_directory)\n",
    "os.chdir('Population/src/core')\n",
    "\n",
    "# trainers.py indicates that the parameters of the sender should be considered to be\n",
    "#   agent.sender.parameters() + agent.object_encoder.parameters()\n",
    "\n",
    "from senders import build_sender\n",
    "\n",
    "# sender_params = {\n",
    "#     # based on \n",
    "#     \"sender_type\": \"recurrent\"\n",
    "#     \"sender_cell\": \"gru\"\n",
    "#     \"sender_type\": \"recurrent\"\n",
    "# }\n",
    "game_params = {\n",
    "    \"objects\": {\"object_type\": \"dummy\"},\n",
    "    \"channel\": {\n",
    "        \"voc_size\": 10, # From default_json/onehotcompositionality_game.json\n",
    "        \"max_len\": 10, # From default_json/onehotcompositionality_game.json\n",
    "    }\n",
    "}\n",
    "sender = build_sender(sender_params, game_params)\n",
    "\n",
    "params, order = count_parameters(sender)\n",
    "parameter_sizes.append({\n",
    "    'title': 'Emergent Communication: Generalization and Overfitting in Lewis Games',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e9035-cc43-450b-a8c5-ab446f34b4f8",
   "metadata": {},
   "source": [
    "## 21. On the Correspondence between Compositionality and Imitation in Emergent Neural Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff044a-28df-4aad-af7a-2759fd3296e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The paper (https://aclanthology.org/2023.findings-acl.787.pdf) \n",
    "says \"The Sender is a single-layer GRU (Cho et al., 2014)\n",
    "containing a fully-connected (FC) layer that maps the input\n",
    "x to its first hidden state (dim=128).\"\n",
    "And also says\n",
    "\"Each input x denotes an object in an “attribute-\n",
    "value world\", where the object has n_{att} attributes,\n",
    "and each attribute takes n_{val} possible values. We\n",
    "represent x by a concatenation of n_{att} one-hot vec-\n",
    "tors, each of dimension {n_val}.\n",
    "...\n",
    "We set n_{att} = 6, n_{val} = 10\"\n",
    "'''\n",
    "class ImitNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(ImitNet, self).__init__()\n",
    "        self.fc = nn.Linear(6 * 10, 128)\n",
    "        self.gru = nn.GRU(128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "       \n",
    "        return self.gru(self.fc(x))\n",
    "\n",
    "params, order = count_parameters(ImitNet())\n",
    "parameter_sizes.append({\n",
    "    'title': 'On the Correspondence between Compositionality and Imitation in Emergent Neural Communication',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c21b7-5e96-4547-bdd2-7b2aa877169f",
   "metadata": {},
   "source": [
    "## 22. Compositionality with Variation Reliably Emerges in Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959397b-70eb-40f6-8a08-598753cb4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paper says\n",
    "\"\"\"\n",
    "The overall architecture used is intentionally similar to Chaabouni et al. (2020); Resnick et al. (2020) and\n",
    "Guo et al. (2021) to allow comparison of results. The sender network is comprised of an embedding layer,\n",
    "linear layer, and a GRU (Cho et al., 2014\n",
    "\"\"\"\n",
    "\n",
    "# Here is the same model we made to mock up Chaabouni (2020),\n",
    "# with parameters changed according to configs/iclr_2023.jsonnet and the paper\n",
    "class VariationNet(nn.Module):\n",
    "\n",
    "    # network structure\n",
    "    def __init__(self):\n",
    "        super(VariationNet, self).__init__()\n",
    "        self.vocab = 26 # 'signal_alphabet_size' from configs/iclr_2023.jsonnet\n",
    "        self.embedding_size = 52 # 'embedding_size' from configs/iclr_2023.jsonnet\n",
    "        self.input_size = 75 # sum of 'n_atoms_per_role' from configs/iclr_2023.jsonnet based on 1-hot vector input\n",
    "        self.fc1   = nn.Linear(self.input_size, self.embedding_size)\n",
    "        self.gru   = nn.GRU(self.vocab, self.embedding_size)\n",
    "        self.fc2   = nn.Linear(self.embedding_size, self.vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        One forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input\n",
    "        '''\n",
    "        hidden = self.fc1(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        return self.fc2(out)\n",
    "\n",
    "\n",
    "params, order = count_parameters(VariationNet())\n",
    "parameter_sizes.append({\n",
    "    'title': 'Compositionality with Variation Reliably Emerges in Neural Networks',\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255da8c-c416-4a46-ab6c-9fc681cb2599",
   "metadata": {},
   "source": [
    "## 23. On the Word Boundaries of Emergent Languages Based on Harris's Articulation Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564fd43-d62f-455c-9139-f23683bb8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paper says\n",
    "# \"We have to define environments, agent architectures, and optimization methods for language emer-\n",
    "# gence simulations. This paper adopts the framework of Chaabouni et al. (2020). ...\n",
    "# We follow Chaabouni et al. (2020) as well for the architecture and optimization method.\"\n",
    "\n",
    "# Same game and architecture as Chaabouni, so inherit param size results from that paper:\n",
    "inherits_from = 'Compositionality and Generalization in Emergent Languages'\n",
    "previous_calculation = [p for p in parameter_sizes if p['title'] == inherits_from][0]\n",
    "params, order = previous_calculation['params'], previous_calculation['order']\n",
    "\n",
    "parameter_sizes.append({\n",
    "    'title': \"On the Word Boundaries of Emergent Languages Based on Harris's Articulation Scheme\",\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "params, order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d5a39-901c-427e-a587-e22a46fec3d8",
   "metadata": {},
   "source": [
    "## 24. Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c8637-1048-419d-b751-ef2cee667cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same game and architecture as Chaabouni, amnd \"On the word boundaries\"\n",
    "# so inherit param size results from Chaabouni. Additional language model\n",
    "# is conceptualised to be part of the receiver.\n",
    "\n",
    "inherits_from = 'Compositionality and Generalization in Emergent Languages'\n",
    "previous_calculation = [p for p in parameter_sizes if p['title'] == inherits_from][0]\n",
    "params, order = previous_calculation['params'], previous_calculation['order']\n",
    "\n",
    "parameter_sizes.append({\n",
    "    'title': \"Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments\",\n",
    "    'params': params,\n",
    "    'order': order\n",
    "})\n",
    "\n",
    "params, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69599ebb-ea51-4d06-ad8e-2e3a7932c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "os.chdir(original_working_directory)\n",
    "\n",
    "pd.DataFrame(parameter_sizes).to_csv('output.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
